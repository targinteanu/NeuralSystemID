{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn import GELU\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37232483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs created: 1419465\n",
      "Input shape : torch.Size([1419465, 32, 7])\n",
      "Output shape: torch.Size([1419465, 16, 7])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the Data ---------------------------------------------------------------------\n",
    "\n",
    "# ------------------------\n",
    "# Load info\n",
    "# ------------------------\n",
    "info_df = pd.read_csv(\"info.csv\")\n",
    "fs = info_df.iloc[0, 5]                  # sampling frequency (Hz)\n",
    "\n",
    "# ------------------------\n",
    "# Load baseline data\n",
    "# ------------------------\n",
    "baseline_df = pd.read_csv(\"baselinedataraw.csv\")\n",
    "baseline_time = baseline_df.iloc[:, 0].values            # time column (seconds)\n",
    "baseline_data = baseline_df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load main data\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"dataraw.csv\")\n",
    "time = df.iloc[:, 0].values            # time column (seconds)\n",
    "data = df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load events\n",
    "# ------------------------\n",
    "events_df = pd.read_csv(\"events.csv\")\n",
    "event_times = events_df.iloc[:, 0].values  # assume first column is event time in seconds\n",
    "event_times = np.sort(event_times)         # ensure sorted\n",
    "\n",
    "# For fast lookup using binary search\n",
    "def count_events_in_window(t, window=0.2):\n",
    "    \"\"\"\n",
    "    Count how many event_times fall in (t - window, t].\n",
    "    Uses bisect for O(log n) search.\n",
    "    \"\"\"\n",
    "    left = bisect.bisect_right(event_times, t - window)\n",
    "    right = bisect.bisect_right(event_times, t)\n",
    "    return right - left\n",
    "\n",
    "# ------------------------\n",
    "# Compute event count for each row\n",
    "# ------------------------\n",
    "event_counts = np.array([count_events_in_window(t, 1.2/fs) for t in time])\n",
    "event_counts = event_counts.reshape(-1, 1)\n",
    "\n",
    "# Append event_counts as an additional input feature\n",
    "data_aug = np.hstack([data, event_counts])\n",
    "# Now each input row has: [original data..., event_count]\n",
    "\n",
    "# ------------------------\n",
    "# Determine outliers\n",
    "# ------------------------\n",
    "threshsd = 3 # standard deviations \n",
    "threshprop = .5 # proportion of features\n",
    "BLmean = np.mean(baseline_data, axis=0)\n",
    "BLstd = np.std(baseline_data, axis=0)\n",
    "BLisout = np.abs(baseline_data - BLmean) > (threshsd * BLstd)\n",
    "BLisnoise = np.sum(BLisout, axis=1) > (threshprop * baseline_data.shape[1])\n",
    "isout = np.abs(data - BLmean) > (threshsd * BLstd)\n",
    "isnoise = np.sum(isout, axis=1) > (threshprop * data.shape[1])\n",
    "\n",
    "# ------------------------\n",
    "# Build input-output pairs using the _ ms rule\n",
    "# ------------------------\n",
    "dt_target = 1/fs      # s\n",
    "dt_tol = 0.15 * dt_target\n",
    "seq_len = 32                   # samples\n",
    "hzn_len = 16                   # samples\n",
    "drow_target = int(dt_target * fs)  # number of rows \n",
    "\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "# create sliding windows\n",
    "def create_windows(data, seq_len=128, horizon=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - seq_len - horizon + 1):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        Y.append(data[(i+seq_len):(i+seq_len+horizon)])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# --- baseline data ---\n",
    "Nbl = len(baseline_df)\n",
    "inputs = []\n",
    "for i in range(Nbl - drow_target):\n",
    "    dt = baseline_time[i+drow_target] - baseline_time[i]\n",
    "    if (abs(dt - dt_target) <= dt_tol) and (not BLisnoise[i]):\n",
    "        inputs.append(baseline_data[i]) \n",
    "    else:\n",
    "        if len(inputs) > seq_len+hzn_len:\n",
    "            x, y = create_windows(inputs, seq_len, hzn_len)\n",
    "            if x is not None:\n",
    "                X_list.append(x)\n",
    "                Y_list.append(y)\n",
    "            inputs = []\n",
    "# catch trailing segment\n",
    "if len(inputs) > seq_len+hzn_len:\n",
    "    x, y = create_windows(inputs, seq_len, hzn_len)\n",
    "    if x is not None:\n",
    "        X_list.append(x)\n",
    "        Y_list.append(y)\n",
    "\n",
    "X = np.concatenate(X_list, axis=0)\n",
    "Y = np.concatenate(Y_list, axis=0)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "print(\"Pairs created:\", len(X))\n",
    "print(\"Input shape :\", X.shape)   \n",
    "print(\"Output shape:\", Y.shape)\n",
    "\n",
    "num_feat = X.shape[-1]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d48702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture for encoder-only, Hzn=1\n",
    "\n",
    "\n",
    "def elu_feature_map(x):\n",
    "    # FAVOR+ feature map\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # project to Q, K, V\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # reshape to multi-head\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "        # apply kernel feature map\n",
    "        q = elu_feature_map(q)\n",
    "        k = elu_feature_map(k)\n",
    "\n",
    "        # normalize to avoid numerical issues\n",
    "        eps = 1e-6\n",
    "\n",
    "        # Compute KV = sum_t (phi(K_t) * V_t)\n",
    "        # (B, num_heads, head_dim, head_dim)\n",
    "        kv = torch.einsum(\"bthd,bthm->bhmd\", k, v)\n",
    "\n",
    "        # Compute normalizer: z = 1 / (sum_t phi(K_t) * 1)\n",
    "        # (B, num_heads, head_dim)\n",
    "        z = 1 / (torch.einsum(\"bthd,bhd->bth\", q, k.sum(dim=1)) + eps)\n",
    "\n",
    "        # Compute output: y_t = (phi(Q_t) * KV) * z_t\n",
    "        # (B, T, num_heads, head_dim)\n",
    "        out = torch.einsum(\"bthd,bhmd->bthm\", q, kv)\n",
    "        out = out * z.unsqueeze(-1)\n",
    "\n",
    "        # merge heads\n",
    "        out = out.reshape(B, T, D)\n",
    "\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class LinearTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, num_heads, dim_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = LinearAttention(dim_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, dim_model),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        attn_out = self.attn(x)\n",
    "        x = self.norm1(x + self.dropout1(attn_out))\n",
    "\n",
    "        # Feedforward block\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of shape (max_len, dim_model)\n",
    "        pe = torch.zeros(max_len, dim_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "\n",
    "        # Divide by log-based frequencies\n",
    "        div_term = torch.exp(torch.arange(0, dim_model, 2) * (-math.log(10000.0) / dim_model))\n",
    "\n",
    "        # Apply sin to even indices, cos to odd\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register as buffer so it's saved with model but not trainable\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, dim_model)\n",
    "        returns: x + positional_encoding[:, :T, :]\n",
    "        \"\"\"\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_model=64, num_heads=4, num_layers=4, dim_ff=128, pos_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project inputs into model dimension\n",
    "        self.input_proj = nn.Linear(dim_in, dim_model)\n",
    "\n",
    "        # Positional embedding\n",
    "        #self.pos_emb = nn.Parameter(torch.randn(1, pos_len, dim_model))\n",
    "        self.pos_emb = SinusoidalPositionalEncoding(dim_model, max_len=pos_len)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True  # lets inputs be (B, T, D)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head for next-step prediction\n",
    "        self.fc_out = nn.Linear(dim_model, dim_in)\n",
    "        # TO DO: try changing this to direct one-shot K-step prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, dim_in)\n",
    "        \"\"\"\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(f\"Expected input ndim=3, got {x.ndim}\")\n",
    "        T = x.size(1)\n",
    "        #if T > self.pos_emb.size(1):\n",
    "        #    raise RuntimeError(f\"Sequence length T={T} exceeds pos_len={self.pos_emb.size(1)}. \"\n",
    "        #                       \"Either increase pos_len or ensure inputs have smaller T.\")\n",
    "        #x = self.input_proj(x) + self.pos_emb[:, :T, :]\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_emb(x)\n",
    "        z = self.encoder(x)\n",
    "        out = self.fc_out(z[:, -1])  # decode final token\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e928221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture for seq2seq\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: causal mask for decoder\n",
    "# ----------------------------\n",
    "def generate_square_subsequent_mask(sz: int, device: torch.device):\n",
    "    \"\"\"Upper-triangular mask with 0 on and below diagonal, -inf above to mask future tokens.\"\"\"\n",
    "    mask = torch.triu(torch.ones((sz, sz), device=device), diagonal=1).bool()\n",
    "    # nn.Transformer modules expect float mask with -inf for masked positions if using add_mask\n",
    "    # but when using boolean mask arguments (src_key_padding_mask / tgt_key_padding_mask) behavior differs.\n",
    "    # We'll return a float mask suitable for use in `attn_mask` (additive).\n",
    "    attn_mask = torch.full((sz, sz), float('-inf'), device=device)\n",
    "    attn_mask[~mask] = 0.0\n",
    "    return attn_mask  # shape (sz, sz)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Seq2Seq Transformer Model\n",
    "# ----------------------------\n",
    "class Seq2SeqTimeSeriesTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,         # number of input features (all features present in encoder input)\n",
    "        dim_out,        # number of features to predict (subset of dim_in)\n",
    "        seq_len=64,\n",
    "        horizon=32,\n",
    "        dim_model=32,\n",
    "        num_heads=2,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dim_ff=64,\n",
    "        dropout=0.1,\n",
    "        #max_positional_len=5000,\n",
    "        device=torch.device('cpu'),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # 1) Input projection: map raw input features -> model dimension\n",
    "        self.input_proj = nn.Linear(dim_in, dim_model)\n",
    "\n",
    "        # 2) Positional embeddings (learned)\n",
    "        #self.pos_emb_enc = nn.Parameter(torch.randn(1, max_positional_len, dim_model))\n",
    "        #self.pos_emb_dec = nn.Parameter(torch.randn(1, max_positional_len, dim_model))\n",
    "        # sinusoidal positional embeddings\n",
    "        self.pos_emb_enc = SinusoidalPositionalEncoding(dim_model, max_len=seq_len)\n",
    "        self.pos_emb_dec = SinusoidalPositionalEncoding(dim_model, max_len=horizon)\n",
    "\n",
    "        # 3) Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # 4) Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # 5) Output projection: map model dimension -> predicted features\n",
    "        self.output_proj = nn.Linear(dim_model, dim_out)\n",
    "\n",
    "        # optional: small linear to initialize first decoder input from encoder summary, if needed\n",
    "        self.dec_init_proj = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self._reset_parameters()\n",
    "        self.to(device)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Initialization similar to PyTorch Transformer\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def encode(self, src):\n",
    "        \"\"\"\n",
    "        src: (batch, seq_len, dim_in)\n",
    "        returns: memory (batch, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        b, t, _ = src.shape\n",
    "        x = self.input_proj(src)                       # (b, t, dim_model)\n",
    "        #x = x + self.pos_emb_enc[:, :t, :]             # add positional embedding\n",
    "        x = self.pos_emb_enc(x)                        # add positional embedding\n",
    "        memory = self.encoder(x)                       # (b, t, dim_model)\n",
    "        return memory\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: (batch, tgt_len, dim_model)  -- already projected + pos emb\n",
    "        memory: encoder outputs (batch, src_len, dim_model)\n",
    "        \"\"\"\n",
    "        out = self.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "        )  # (b, tgt_len, dim_model)\n",
    "        return out\n",
    "\n",
    "    def forward(self, src, tgt_in):\n",
    "        \"\"\"\n",
    "        Forward pass used in training with teacher forcing.\n",
    "\n",
    "        src: (batch, seq_len, dim_in)\n",
    "        tgt_in: (batch, tgt_len, dim_out) -- ground truth future sequence shifted right\n",
    "                Example: if horizon = H and dim_out=F,\n",
    "                tgt_in[:, 0, :] should be a start token (e.g., zeros)\n",
    "                tgt_in[:, 1:, :] are ground-truth up to H-1 steps (teacher forcing).\n",
    "        Returns:\n",
    "            logits: (batch, tgt_len, dim_out)\n",
    "        \"\"\"\n",
    "        device = src.device\n",
    "        b, src_len, _ = src.shape\n",
    "        _, tgt_len, _ = tgt_in.shape\n",
    "        #assert src_len <= self.pos_emb_enc.shape[1], \"increase max_positional_len\"\n",
    "        #assert tgt_len <= self.pos_emb_dec.shape[1], \"increase max_positional_len\"\n",
    "\n",
    "        # 1) Encode\n",
    "        memory = self.encode(src)  # (b, src_len, dim_model)\n",
    "\n",
    "        # 2) Prepare decoder input embeddings: we project the provided tgt_in (dim_out) into dim_model\n",
    "        #    Common pattern: during training, feed the true previous outputs (teacher forcing).\n",
    "        #    Because tgt_in contains actual feature values (not model embeddings), we map them\n",
    "        #    into model space with a small linear layer (re-using input_proj for simplicity if dims match).\n",
    "        #    Here, to keep model flexible, we'll use a small linear (input_proj_dec) implemented on the fly.\n",
    "        # Simple approach: expand dim_out -> dim_model with a linear\n",
    "        # For simplicity re-use input_proj if dim_in == dim_out; otherwise make a quick linear on the fly:\n",
    "        if self.dim_in == self.dim_out:\n",
    "            # reuse input projection if dims align\n",
    "            tgt_emb = self.input_proj(tgt_in)            # (b, tgt_len, dim_model)\n",
    "        else:\n",
    "            # lightweight linear mapping for decoder inputs (not saved as parameter to keep API simple)\n",
    "            # but better to declare a module if used heavily; here we allocate on the fly for clarity\n",
    "            # (we'll do a proper parametrized projection to avoid re-creating params each forward)\n",
    "            if not hasattr(self, 'tgt_input_proj'):\n",
    "                self.tgt_input_proj = nn.Linear(self.dim_out, self.dim_model).to(self.device)\n",
    "            tgt_emb = self.tgt_input_proj(tgt_in)\n",
    "\n",
    "        # add positional embeddings\n",
    "        #tgt_emb = tgt_emb + self.pos_emb_dec[:, :tgt_len, :]\n",
    "        tgt_emb = self.pos_emb_dec(tgt_emb)\n",
    "\n",
    "        # 3) Create causal mask for decoder self-attention\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_len, device=device)  # (tgt_len, tgt_len)\n",
    "\n",
    "        # 4) Run decoder: decoder expects target embeddings + encoder memory\n",
    "        dec_out = self.decode(\n",
    "            tgt=tgt_emb,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "        )  # (b, tgt_len, dim_model)\n",
    "\n",
    "        # 5) Project decoder outputs to predicted feature space\n",
    "        logits = self.output_proj(dec_out)  # (b, tgt_len, dim_out)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, src, horizon=None, start_token=None):\n",
    "        \"\"\"\n",
    "        Autoregressive generation / inference.\n",
    "\n",
    "        src: (batch, seq_len, dim_in)\n",
    "        horizon: number of steps to generate (defaults to self.horizon)\n",
    "        start_token: (dim_out,) or (batch, dim_out) initial token fed to decoder as t=0.\n",
    "                     If None, zeros are used.\n",
    "\n",
    "        Returns:\n",
    "            generated: (batch, horizon, dim_out)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = src.device\n",
    "        b = src.shape[0]\n",
    "        horizon = self.horizon if horizon is None else horizon\n",
    "\n",
    "        # 1) encode\n",
    "        memory = self.encode(src)  # (b, src_len, dim_model)\n",
    "\n",
    "        # 2) prepare initial decoder input (t=0)\n",
    "        if start_token is None:\n",
    "            cur_input = torch.zeros((b, 1, self.dim_out), device=device)  # (b, 1, dim_out)\n",
    "        else:\n",
    "            # allow vector or batch vector\n",
    "            st = torch.tensor(start_token, device=device, dtype=src.dtype)\n",
    "            if st.dim() == 1:\n",
    "                st = st.unsqueeze(0).expand(b, -1)\n",
    "            cur_input = st.unsqueeze(1)  # (b, 1, dim_out)\n",
    "\n",
    "        generated = []\n",
    "        # autoregressive loop\n",
    "        for t in range(horizon):\n",
    "            # build tgt_in as the sequence of already generated tokens for this batch\n",
    "            if len(generated) == 0:\n",
    "                tgt_in = cur_input  # (b, 1, dim_out)\n",
    "            else:\n",
    "                # stack previously generated tokens\n",
    "                prev = torch.cat(generated, dim=1)  # (b, t, dim_out)\n",
    "                tgt_in = torch.cat([cur_input, prev], dim=1)  # (b, t+1, dim_out)\n",
    "\n",
    "            # forward through model (teacher forcing not applied)\n",
    "            logits = self.forward(src, tgt_in)  # (b, t+1, dim_out)\n",
    "            # take the last step's predictions as next token\n",
    "            next_token = logits[:, -1:, :]  # (b, 1, dim_out)\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # note: optional sampling or temperature can be applied here for stochastic outputs\n",
    "\n",
    "        gen = torch.cat(generated, dim=1)  # (b, horizon, dim_out)\n",
    "        return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3189cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "test_size=0.7\n",
    "batch_size = 32\n",
    "\n",
    "#model = TimeSeriesTransformer(dim_in=num_feat, dim_model=8, num_heads=2, num_layers=2, dim_ff=16, pos_len=seq_len)\n",
    "model = Seq2SeqTimeSeriesTransformer(dim_in=num_feat, dim_out=num_feat, seq_len=seq_len, horizon=hzn_len, dim_model=8, num_heads=2, num_encoder_layers=2, num_decoder_layers=2, dim_ff=16)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train / test\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size, random_state=42)\n",
    "train_N = int((1 - test_size) * len(X))\n",
    "X_train = X[:train_N]\n",
    "Y_train = Y[:train_N]\n",
    "X_test = X[train_N:]\n",
    "Y_test = Y[train_N:]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "all_loader = DataLoader(TensorDataset(X, Y), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca1267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data baseline characteristics as reference for loss \n",
    "mean_y = Y_train.mean(dim=0)\n",
    "std_y = Y_train.std(dim=0)\n",
    "var_y = std_y ** 2\n",
    "var_per_feat = np.var(Y_train.numpy(), axis=0)  # redundant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37bda167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, loss_fn, device, teacher_forcing_ratio=1.0):\n",
    "    \"\"\"\n",
    "    One epoch of training with optional teacher forcing ratio.\n",
    "    teacher_forcing_ratio=1.0 -> always use ground truth as decoder input (fastest, stable)\n",
    "    teacher_forcing_ratio=0.0 -> always use model predictions as previous inputs (full AR)\n",
    "    Usually we use teacher forcing during training; for robustness you can mix.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in dataloader:\n",
    "        xb = xb.to(device).float()   # (batch, seq_len, dim_in)\n",
    "        yb = yb.to(device).float()   # (batch, horizon, dim_out)\n",
    "\n",
    "        b, H, D_out = yb.shape\n",
    "\n",
    "        # Build tgt_in for teacher forcing:\n",
    "        # tgt_in shape: (batch, H, dim_out)\n",
    "        # convention: tgt_in[:, 0, :] is \"start token\" (zeros)\n",
    "        start = torch.zeros((b, 1, D_out), device=device, dtype=xb.dtype)\n",
    "        if teacher_forcing_ratio >= 1.0:\n",
    "            tgt_in = torch.cat([start, yb[:, :-1, :]], dim=1)\n",
    "            # forward\n",
    "            logits = model(xb, tgt_in)   # (batch, H, dim_out)\n",
    "        else:\n",
    "            # scheduled sampling: mix ground truth and previous predictions\n",
    "            # simple implementation: step through horizon building tgt_in progressively\n",
    "            preds = []\n",
    "            prev = start\n",
    "            for t in range(H):\n",
    "                # build current input sequence\n",
    "                if t == 0:\n",
    "                    cur_tgt = prev\n",
    "                else:\n",
    "                    cur_tgt = torch.cat([start, torch.cat(preds, dim=1)], dim=1)\n",
    "                logits_t = model(xb, cur_tgt)  # (batch, t+1, dim_out)\n",
    "                next_pred = logits_t[:, -1:, :]  # (b,1,d)\n",
    "                # decide whether to teacher-force for next step\n",
    "                if torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    chosen = yb[:, t:t+1, :]\n",
    "                else:\n",
    "                    chosen = next_pred.detach()\n",
    "                preds.append(chosen)\n",
    "            logits = torch.cat(preds, dim=1)\n",
    "\n",
    "        loss = loss_fn(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.shape[0]\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def validate_single_step(model, test_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)        # [B, seq_len, dim_in]\n",
    "            y = y.to(device)        # [B, horizon, dim_out]\n",
    "\n",
    "            \"\"\"\n",
    "            _, pred = model(x, target_seq_len=1)  # pred: [B, 1, dim_out]\n",
    "            pred = pred[:, -1, :]                  # → [B, dim_out]\n",
    "            \"\"\"\n",
    "            pred = model.generate(x, horizon=y.size(1))  # [B, horizon, dim_out]\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return total_loss / len(test_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0364698a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m epoch_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_single_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 66\u001b[0m, in \u001b[0;36mvalidate_single_step\u001b[0;34m(model, test_loader, device, criterion)\u001b[0m\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)        \u001b[38;5;66;03m# [B, horizon, dim_out]\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m_, pred = model(x, target_seq_len=1)  # pred: [B, 1, dim_out]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mpred = pred[:, -1, :]                  # → [B, dim_out]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, horizon, dim_out]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, y)\n\u001b[1;32m     69\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 217\u001b[0m, in \u001b[0;36mSeq2SeqTimeSeriesTransformer.generate\u001b[0;34m(self, src, horizon, start_token)\u001b[0m\n\u001b[1;32m    214\u001b[0m     tgt_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cur_input, prev], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (b, t+1, dim_out)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# forward through model (teacher forcing not applied)\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_in\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, t+1, dim_out)\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# take the last step's predictions as next token\u001b[39;00m\n\u001b[1;32m    219\u001b[0m next_token \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# (b, 1, dim_out)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 164\u001b[0m, in \u001b[0;36mSeq2SeqTimeSeriesTransformer.forward\u001b[0;34m(self, src, tgt_in)\u001b[0m\n\u001b[1;32m    161\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m generate_square_subsequent_mask(tgt_len, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (tgt_len, tgt_len)\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# 4) Run decoder: decoder expects target embeddings + encoder memory\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, tgt_len, dim_model)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# 5) Project decoder outputs to predicted feature space\u001b[39;00m\n\u001b[1;32m    171\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(dec_out)  \u001b[38;5;66;03m# (b, tgt_len, dim_out)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 108\u001b[0m, in \u001b[0;36mSeq2SeqTimeSeriesTransformer.decode\u001b[0;34m(self, tgt, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, memory_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    tgt: (batch, tgt_len, dim_model)  -- already projected + pos emb\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    memory: encoder outputs (batch, src_len, dim_model)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, tgt_len, dim_model)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:628\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    625\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 628\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    640\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:1131\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m   1123\u001b[0m         x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n\u001b[1;32m   1124\u001b[0m     )\n\u001b[1;32m   1125\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\n\u001b[1;32m   1126\u001b[0m         x\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(\n\u001b[1;32m   1128\u001b[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[1;32m   1129\u001b[0m         )\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[0;32m-> 1131\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:1176\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1176\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout3(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_val_loss = validate_single_step(model, test_loader, torch.device('cpu'), criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f44fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 425839\n",
      "Batch size: 32\n",
      "Batches/epoch: 13308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# --- validate ---\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m epoch_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_single_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(epoch_val_loss)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m — train_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 66\u001b[0m, in \u001b[0;36mvalidate_single_step\u001b[0;34m(model, test_loader, device, criterion)\u001b[0m\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)        \u001b[38;5;66;03m# [B, horizon, dim_out]\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m_, pred = model(x, target_seq_len=1)  # pred: [B, 1, dim_out]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mpred = pred[:, -1, :]                  # → [B, dim_out]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, horizon, dim_out]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, y)\n\u001b[1;32m     69\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 217\u001b[0m, in \u001b[0;36mSeq2SeqTimeSeriesTransformer.generate\u001b[0;34m(self, src, horizon, start_token)\u001b[0m\n\u001b[1;32m    214\u001b[0m     tgt_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cur_input, prev], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (b, t+1, dim_out)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# forward through model (teacher forcing not applied)\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_in\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, t+1, dim_out)\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# take the last step's predictions as next token\u001b[39;00m\n\u001b[1;32m    219\u001b[0m next_token \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# (b, 1, dim_out)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 136\u001b[0m, in \u001b[0;36mSeq2SeqTimeSeriesTransformer.forward\u001b[0;34m(self, src, tgt_in)\u001b[0m\n\u001b[1;32m    131\u001b[0m _, tgt_len, _ \u001b[38;5;241m=\u001b[39m tgt_in\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#assert src_len <= self.pos_emb_enc.shape[1], \"increase max_positional_len\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m#assert tgt_len <= self.pos_emb_dec.shape[1], \"increase max_positional_len\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# 1) Encode\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, src_len, dim_model)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# 2) Prepare decoder input embeddings: we project the provided tgt_in (dim_out) into dim_model\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m#    Common pattern: during training, feed the true previous outputs (teacher forcing).\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#    Because tgt_in contains actual feature values (not model embeddings), we map them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Simple approach: expand dim_out -> dim_model with a linear\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# For simplicity re-use input_proj if dim_in == dim_out; otherwise make a quick linear on the fly:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_in \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_out:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# reuse input projection if dims align\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 100\u001b[0m, in \u001b[0;36mSeq2SeqTimeSeriesTransformer.encode\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#x = x + self.pos_emb_enc[:, :t, :]             # add positional embedding\u001b[39;00m\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb_enc(x)                        \u001b[38;5;66;03m# add positional embedding\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m                       \u001b[38;5;66;03m# (b, t, dim_model)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m memory\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:902\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[1;32m    899\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(\n\u001b[1;32m    900\u001b[0m             src_mask, src_key_padding_mask, src\n\u001b[1;32m    901\u001b[0m         )\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\u001b[39;00m\n\u001b[1;32m    926\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 4: Train the Model\n",
    "train_size = len(train_loader.dataset)\n",
    "steps_per_epoch = math.ceil(train_size / batch_size)\n",
    "print(\"Train samples:\", train_size)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Batches/epoch:\", steps_per_epoch)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 100 # max\n",
    "patience = 10\n",
    "best_val = float('inf')\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    epoch_train_loss = train_epoch(model, train_loader, optimizer, criterion, torch.device('cpu'), teacher_forcing_ratio=1.0)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    epoch_val_loss = validate_single_step(model, test_loader, torch.device('cpu'), criterion)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "    # --- Early stopping ---\n",
    "    if epoch_val_loss < best_val - 1e-9:\n",
    "        best_val = epoch_val_loss\n",
    "        no_improve = 0\n",
    "        # Optionally save best model:\n",
    "        # torch.save(model.state_dict(), \"neural_network_pytorch.pth\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs — stopping early at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# After loop: plot train/val loss to inspect convergence\n",
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "# model.load_state_dict(torch.load(\"neural_network_pytorch.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the Model on Test Data\n",
    "\n",
    "Y_pred = []\n",
    "Y_test1 = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        if y_pred.shape[0] == seq_len:\n",
    "            Y_pred.append(y_pred)\n",
    "            Y_test1.append(y_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_np = np.array(Y_pred)\n",
    "Y_pred_np = Y_pred_np.reshape(-1, num_feat)\n",
    "Y_test_np = Y_test.numpy()\n",
    "Y_test1_np = np.array(Y_test1)\n",
    "Y_test1_np = Y_test1_np.reshape(-1, num_feat)\n",
    "\n",
    "Y_null_all_np = X.numpy()[:, -1, :Y.shape[1]]\n",
    "Y_null_test_np = X_test.numpy()[:, -1, :Y.shape[1]]\n",
    "\n",
    "MSE_per_feat = np.mean((Y_test1_np - Y_pred_np) ** 2, axis=0)\n",
    "MSE_per_feat_null = np.mean((Y_test_np - Y_null_test_np) ** 2, axis=0)\n",
    "feats = np.arange(1, Y.shape[1]+1)\n",
    "barwid = .35\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(feats - barwid, var_per_feat, width=barwid, label='Output Variance')\n",
    "plt.bar(feats, MSE_per_feat_null, width=barwid, label='Null MSE')\n",
    "plt.bar(feats + barwid, MSE_per_feat, width=barwid, label='Test MSE')\n",
    "plt.xlabel('Output Feature')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Output Feature Variance vs Test MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046469d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_np = X.numpy()\n",
    "Y_all_np = Y.numpy()\n",
    "\n",
    "Y_all_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in all_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        Y_all_pred.append(y_pred)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test+Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_all_np = np.array(Y_all_pred)\n",
    "Y_pred_all_np = Y_pred_all_np.reshape(-1, num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations \n",
    "simdur = int(0.2 * fs) # samples \n",
    "plotdomain = 1000 * np.array([-1, 1]) + train_N\n",
    "\n",
    "Ysim = []\n",
    "i0 = plotdomain[0]\n",
    "model.eval()\n",
    "while i0+simdur < plotdomain[1]:\n",
    "    xi = torch.tensor(X_all_np[i0, :, :].reshape(1,-1,num_feat), dtype=torch.float32)\n",
    "    for i in range(simdur):\n",
    "        with torch.no_grad():\n",
    "            yi = model(xi).numpy().flatten()\n",
    "        Ysim.append(yi)\n",
    "        # prepare next input\n",
    "        if i < simdur - 1:\n",
    "            #event_count_next = X_all_np[i0 + i + 1, -1]  # keep using original event count\n",
    "            #xi = torch.tensor(np.hstack([yi, event_count_next]).reshape(1, -1), dtype=torch.float32)\n",
    "            xi = torch.tensor(np.vstack([xi[0, 1:, :], yi]).reshape(1,-1,X.shape[-1]), dtype=torch.float32)\n",
    "    i0 += simdur\n",
    "    print(\"Simulating:\", (i0-plotdomain[0])/(plotdomain[1]-plotdomain[0]), \" complete.\" )\n",
    "\n",
    "Ysim = np.array(Ysim)\n",
    "plotxval = np.arange(len(Ysim)) + plotdomain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show several examples \n",
    "\n",
    "iMSE = np.argsort(MSE_per_feat)\n",
    "iVAR = np.argsort(var_per_feat)\n",
    "iLRN = np.argsort(MSE_per_feat / var_per_feat)\n",
    "iToPlot = [iMSE[:2], iMSE[-2:], iVAR[:2], iVAR[-2:], iLRN[:2], iLRN[-2:]]\n",
    "iToPlot = list(set([i for sublist in iToPlot for i in sublist]))\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "iPlot = 1\n",
    "for i in iToPlot:\n",
    "    plt.subplot(len(iToPlot), 1, iPlot)\n",
    "    plt.plot(Y_all_np[:, i], label='True')\n",
    "    plt.plot(Y_pred_all_np[:, i], label='Predicted', linestyle='--')\n",
    "    plt.plot(Y_null_all_np[:, i], label='Null', linestyle=':')\n",
    "    plt.plot(plotxval, Ysim[:,i], label='Simulated', linestyle='-.')\n",
    "    plt.xlim(plotdomain)\n",
    "\n",
    "    # set the y limits to be slightly larger than the min/max of true values in the plotdomain\n",
    "    y_min = np.min(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_max = np.max(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "    \n",
    "    plt.title(f'Feature {i+1} - MSE: {MSE_per_feat[i]:.4f}, VAR: {var_per_feat[i]:.4f}')\n",
    "    plt.legend(loc='upper right')\n",
    "    iPlot += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
