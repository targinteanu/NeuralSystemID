function [lgraph, dlnet, numLearnables] = DLN_BasalGanglia_v1(numChan, N, showplot)
% Create Deep Learning Network Architecture
% Network was designed based on the structure of the basal ganglia. 
% Script for creating the layers for a deep learning network with the following 
% properties:
%
% 
%  Number of layers: 22
%  Number of connections: 22
%
% 
% Run the script to create the layers in the output variable |lgraph|.
% 
% To learn more, see <matlab:helpview('deeplearning','generate_matlab_code') 
% Generate MATLAB Code From Deep Network Designer>.
% 
% Auto-generated by MATLAB on 08-Jan-2025 15:54:03
% 
% Inputs: 
%   numChan: number of ECoG channels. This determines the dimension of
%            input and output. Default = 63, i.e. 21x3 grid with no
%            missing channels.
%   N: scaling factor that can proportionally increase the dimension of all
%      layers except input and output. Default = 1
%   showplot: if true [default], lgraph will be plotted in a new figure
% 
% Outputs: 
%   lgraph: LayerGraph object 
%   dlnet: dlnetwork object without output (regression) layer 
%   numLearnables: # of learnables 
% 
%% handle incomplete inputs 
if nargin < 3
    showplot = true;
    if nargin < 2
        N = [];
        if nargin < 1
            numChan = [];
        end
    end
end
if isempty(numChan)
    numChan = 63;
end
if isempty(N)
    N = 1;
end
%% Create Layer Graph
% Create the layer graph variable to contain the network layers.

lgraph = layerGraph();
%% Add Layer Branches
% Add the branches of the network to the layer graph. Each branch is a linear 
% array of layers.

tempLayers = [
    sequenceInputLayer(numChan,"Name","ECoG")
    fullyConnectedLayer(N*207,"Name","fcCortexInput")
    tanhLayer("Name","actCortexInput")
    fullyConnectedLayer(N*207,"Name","fcStriatum")
    tanhLayer("Name","actStriatum")
    fullyConnectedLayer(N*30,"Name","fcGPinput")
    tanhLayer("Name","actGPinput")];
lgraph = addLayers(lgraph,tempLayers);

tempLayers = [
    fullyConnectedLayer(N*4,"Name","fcIndirectInput")
    tanhLayer("Name","actIndirectInput")
    fullyConnectedLayer(N*1,"Name","fcSTNinput")
    tanhLayer("Name","actSTNinput")
    fullyConnectedLayer(N*6,"Name","fcSTNoutput")
    tanhLayer("Name","actSTNoutput")
    fullyConnectedLayer(N*3,"Name","fcIndirectOutput")
    tanhLayer("Name","actIndirectOutput")];
lgraph = addLayers(lgraph,tempLayers);

tempLayers = [
    concatenationLayer(1,2,"Name","catDirectIndirect")
    fullyConnectedLayer(N*10,"Name","fcGPout")
    tanhLayer("Name","actGPout")
    fullyConnectedLayer(N*250,"Name","fcThalamusVA")
    tanhLayer("Name","actThalamusVA")
    fullyConnectedLayer(numChan,"Name","fcCortexOutput")
    regressionLayer("Name","regressionoutput")];
lgraph = addLayers(lgraph,tempLayers);

% clean up helper variable
clear tempLayers;
%% Connect Layer Branches
% Connect all the branches of the network to create the network graph.

lgraph = connectLayers(lgraph,"actGPinput","fcIndirectInput");
lgraph = connectLayers(lgraph,"actGPinput","catDirectIndirect/in1");
lgraph = connectLayers(lgraph,"actIndirectOutput","catDirectIndirect/in2");

%% dlnet and # of learnables 

% for some reason, matlab doesn't want to output the number of learnables
% of the lgraph, but it will do so for the dlnet 
% and for some reason, conversion does not work unless the regressionoutput
% layer is removed 
dlnet = dlnetwork(removeLayers(lgraph, 'regressionoutput'));
numLearnables = [dlnet.Learnables.Value]; % cell array for each layer 
numLearnables = arrayfun(@(l) numel(l{:}), numLearnables); % total # at each layer
numLearnables = sum(numLearnables); % grand total # 

%% Plot Layers

if showplot
    figure;
    plot(lgraph);
end

end