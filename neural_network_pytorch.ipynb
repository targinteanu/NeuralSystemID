{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn import GELU\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Neural Network Architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37232483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the Data ---------------------------------------------------------------------\n",
    "# ------------------------\n",
    "# Load main data\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "time = df.iloc[:, 0].values            # time column (seconds)\n",
    "data = df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load events\n",
    "# ------------------------\n",
    "events_df = pd.read_csv(\"events.csv\")\n",
    "event_times = events_df.iloc[:, 0].values  # assume first column is event time in seconds\n",
    "event_times = np.sort(event_times)         # ensure sorted\n",
    "\n",
    "# For fast lookup using binary search\n",
    "import bisect\n",
    "\n",
    "def count_events_in_window(t, window=0.2):\n",
    "    \"\"\"\n",
    "    Count how many event_times fall in (t - window, t].\n",
    "    Uses bisect for O(log n) search.\n",
    "    \"\"\"\n",
    "    left = bisect.bisect_right(event_times, t - window)\n",
    "    right = bisect.bisect_right(event_times, t)\n",
    "    return right - left\n",
    "\n",
    "# ------------------------\n",
    "# Compute event count for each row\n",
    "# ------------------------\n",
    "event_counts = np.array([count_events_in_window(t) for t in time])\n",
    "\n",
    "# Append event_counts as an additional input feature\n",
    "data_aug = np.hstack([data, event_counts.reshape(-1, 1)])\n",
    "# Now each input row has: [original data..., event_count]\n",
    "\n",
    "# ------------------------\n",
    "# Build input-output pairs using the 3-row / 3 ms rule\n",
    "# ------------------------\n",
    "dt_target = 0.003      # 3 ms\n",
    "dt_tol = 0.0005        # Â±0.5 ms\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "N = len(df)\n",
    "\n",
    "for i in range(N - 3):\n",
    "    dt = time[i+3] - time[i]\n",
    "    if abs(dt - dt_target) <= dt_tol:\n",
    "        inputs.append(data_aug[i])   # augmented input with event_count\n",
    "        outputs.append(data[i+3])    # output is ONLY the data (no event count)\n",
    "\n",
    "X = torch.tensor(inputs, dtype=torch.float32)\n",
    "Y = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "print(\"Pairs created:\", len(X))\n",
    "print(\"Input shape :\", X.shape)   # features + 1\n",
    "print(\"Output shape:\", Y.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3189cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the Model, Loss Function, and Optimizer\n",
    "batch_size = 64\n",
    "input_size = X.shape[1] # set input_size equal to the width of tensor X\n",
    "output_size = Y.shape[1] # set output_size equal to the width of tensor Y\n",
    "hidden_size = 128\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data baseline characteristics as reference for loss \n",
    "mean_y = Y_train.mean(dim=0)\n",
    "std_y = Y_train.std(dim=0)\n",
    "var_y = std_y ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f44fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Model\n",
    "model.train()\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the Model on Test Data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        total_loss += loss.item() * X_batch.size(0)  # sum up batch loss\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"Baseline STD: {std_y.mean().item():.4f}\")\n",
    "rmse = math.sqrt(avg_loss)\n",
    "print(f\"Test RMSE: {rmse:.4f} | Baseline RMSE: {math.sqrt(var_y.mean().item()):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
