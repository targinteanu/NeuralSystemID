{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn import GELU\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14dc252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Neural Network Architecture\n",
    "\n",
    "# original: all fully connected \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# updated model with conv1d layers\n",
    "class PairedDualConvNet(nn.Module):\n",
    "    def __init__(self, input_dim=101, output_dim=100, group_size=7, num_groups=7):\n",
    "        super().__init__()\n",
    "        assert input_dim >= 98, \"Model assumes at least 98 inputs for paired operations.\"\n",
    "\n",
    "        C1 = 8\n",
    "        C2 = 8\n",
    "\n",
    "        self.num_groups = num_groups        # 7 groups\n",
    "        self.group_size = group_size      # each with 7 pairs\n",
    "        self.num_pairs = num_groups*group_size            # first 98 inputs → 49 pairs\n",
    "        self.paired_len = 2            # each pair has 2 elements\n",
    "        self.used_for_pairing = self.num_pairs * self.paired_len\n",
    "        self.leftover_dim = input_dim - self.used_for_pairing\n",
    "\n",
    "        # -----------------------------\n",
    "        # Stage 1: Conv over each pair\n",
    "        # -----------------------------\n",
    "        self.pair_conv = nn.Conv1d(1, C1, kernel_size=2)\n",
    "        # Stage 1b: map C1 → 2 per pair\n",
    "        self.pair_linear = nn.Conv1d(C1, 2, kernel_size=1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Stage 2: Branch A & B convs (2 → C2)\n",
    "        # -----------------------------\n",
    "        self.convA = nn.Conv1d(2, C2, kernel_size=group_size, padding=0)\n",
    "        self.convB = nn.Conv1d(2, C2, kernel_size=num_groups, padding=0)\n",
    "\n",
    "        # MLP input size: 49*C2 from branch A + 49*C2 from branch B + leftover features\n",
    "        mlp_in = (num_groups * C2) + (group_size * C2) + self.leftover_dim\n",
    "        self.fc1 = nn.Linear(mlp_in, 8)\n",
    "        self.fc2 = nn.Linear(8, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)  # output 100 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Split input\n",
    "        # -----------------------------\n",
    "        x_pairs = x[:, :self.used_for_pairing]   # (B, 98)\n",
    "        x_leftover = x[:, self.used_for_pairing:]  # (B, 3)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Stage 1: form pairs & conv\n",
    "        # -----------------------------\n",
    "        pairs = x_pairs.view(B, self.num_pairs, self.paired_len)  # (B, 49, 2)\n",
    "        pairs_for_conv = pairs.reshape(B * self.num_pairs, 1, self.paired_len)  # (B*49,1,2)\n",
    "        p = self.pair_conv(pairs_for_conv)  # (B*49, C1, 1)\n",
    "        p = F.gelu(p)\n",
    "        p = p.view(B, self.num_pairs, -1).permute(0, 2, 1)  # (B, C1, 49)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Stage 1b: reduce 16 → 2 per pair\n",
    "        # -----------------------------\n",
    "        p = self.pair_linear(p)  # (B, 2, 49)\n",
    "        p = F.gelu(p)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Stage 2A: within-group-of-7 conv\n",
    "        # -----------------------------\n",
    "        num_groups = self.num_groups\n",
    "        group_len = self.group_size\n",
    "        p_groups = p.view(B, 2, num_groups, group_len)          # (B, 2, 7, 7)\n",
    "        p_groups = p_groups.permute(0, 2, 1, 3).contiguous()    # (B,7,2,7)\n",
    "        p_groups = p_groups.view(B*num_groups, 2, group_len)    # (B*7,2,7)\n",
    "        a = self.convA(p_groups)                                 # (B*7, C2, 7)\n",
    "        a = F.gelu(a)\n",
    "        a = a.view(B, num_groups * a.shape[1])       # (B,7*C2)\n",
    "        #a_flat = a.view(B, -1)                                   # (B, 49*C2)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Stage 2B: spaced-by-7 conv\n",
    "        # -----------------------------\n",
    "        p_threads = p.view(B, 2, num_groups, group_len)         # (B,2,7,7)\n",
    "        p_threads = p_threads.permute(0, 3, 1, 2).contiguous()  # (B,7,2,7)\n",
    "        p_threads = p_threads.view(B*group_len, 2, num_groups)  # (B*7,2,7)\n",
    "        b = self.convB(p_threads)                                # (B*7, C2, 7)\n",
    "        b = F.gelu(b)\n",
    "        b = b.view(B, group_len * b.shape[1])        # (B,7*C2)\n",
    "        #b_flat = b.view(B, -1)                                   # (B,49*C2)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Concatenate and MLP\n",
    "        # -----------------------------\n",
    "        h = torch.cat([a, b, x_leftover], dim=1)       # (B, 1571)\n",
    "        h = F.gelu(self.fc1(h))\n",
    "        h = F.gelu(self.fc2(h))\n",
    "        out = self.fc3(h)                                        # (B,100)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37232483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs created: 374997\n",
      "Input shape : torch.Size([374997, 101])\n",
      "Output shape: torch.Size([374997, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/rd6zg9vd5v710s38rhy7y8jc0000gn/T/ipykernel_3737/1909333865.py:86: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  X = torch.tensor(inputs, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the Data ---------------------------------------------------------------------\n",
    "\n",
    "# ------------------------\n",
    "# Load info\n",
    "# ------------------------\n",
    "info_df = pd.read_csv(\"info.csv\")\n",
    "fs = info_df.iloc[0, 5]                  # sampling frequency (Hz)\n",
    "\n",
    "# ------------------------\n",
    "# Load baseline data\n",
    "# ------------------------\n",
    "baseline_df = pd.read_csv(\"baselinedata.csv\")\n",
    "baseline_time = baseline_df.iloc[:, 0].values            # time column (seconds)\n",
    "baseline_data = baseline_df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load main data\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "time = df.iloc[:, 0].values            # time column (seconds)\n",
    "data = df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load events\n",
    "# ------------------------\n",
    "events_df = pd.read_csv(\"events.csv\")\n",
    "event_times = events_df.iloc[:, 0].values  # assume first column is event time in seconds\n",
    "event_times = np.sort(event_times)         # ensure sorted\n",
    "\n",
    "# For fast lookup using binary search\n",
    "def count_events_in_window(t, window=0.2):\n",
    "    \"\"\"\n",
    "    Count how many event_times fall in (t - window, t].\n",
    "    Uses bisect for O(log n) search.\n",
    "    \"\"\"\n",
    "    left = bisect.bisect_right(event_times, t - window)\n",
    "    right = bisect.bisect_right(event_times, t)\n",
    "    return right - left\n",
    "\n",
    "# ------------------------\n",
    "# Compute event count for each row\n",
    "# ------------------------\n",
    "event_counts = np.array([count_events_in_window(t) for t in time])\n",
    "event_counts = event_counts.reshape(-1, 1)\n",
    "\n",
    "# Append event_counts as an additional input feature\n",
    "data_aug = np.hstack([data, event_counts])\n",
    "# Now each input row has: [original data..., event_count]\n",
    "\n",
    "# ------------------------\n",
    "# Build input-output pairs using the _ ms rule\n",
    "# ------------------------\n",
    "dt_target = 0.003      # ms\n",
    "dt_tol = 0.0005        # ±0.5 ms\n",
    "drow_target = int(dt_target * fs)  # number of rows corresponding to 3 ms\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\"\"\"\n",
    "# --- baseline data ---\n",
    "Nbl = len(baseline_df)\n",
    "nbl = int(math.floor(.8 * Nbl))\n",
    "baseline_data_aug = np.hstack([baseline_data, np.zeros((Nbl, 1))])\n",
    "for i in range(nbl - drow_target):\n",
    "    dt = baseline_time[i+drow_target] - baseline_time[i]\n",
    "    if abs(dt - dt_target) <= dt_tol:\n",
    "        inputs.append(baseline_data_aug[i])   # baseline input (zero event count)\n",
    "        outputs.append(baseline_data[i+drow_target])    # output is ONLY the data (no event count)\n",
    "\"\"\"\n",
    "# --- main data ---\n",
    "N = len(df)\n",
    "for i in range(N - drow_target):\n",
    "    dt = time[i+drow_target] - time[i]\n",
    "    if abs(dt - dt_target) <= dt_tol:\n",
    "        inputs.append(data_aug[i])   # augmented input with event_count\n",
    "        outputs.append(data[i+drow_target])    # output is ONLY the data (no event count)\n",
    "\"\"\"\n",
    "# --- baseline data ---\n",
    "for i in range(nbl, Nbl - drow_target):\n",
    "    dt = baseline_time[i+drow_target] - baseline_time[i]\n",
    "    if abs(dt - dt_target) <= dt_tol:\n",
    "        inputs.append(baseline_data_aug[i])   # baseline input (zero event count)\n",
    "        outputs.append(baseline_data[i+drow_target])    # output is ONLY the data (no event count)\n",
    "\"\"\"\n",
    "\n",
    "X = torch.tensor(inputs, dtype=torch.float32)\n",
    "Y = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "print(\"Pairs created:\", len(X))\n",
    "print(\"Input shape :\", X.shape)   # features + 1\n",
    "print(\"Output shape:\", Y.shape)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3189cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the Model, Loss Function, and Optimizer\n",
    "test_size=0.2\n",
    "batch_size = 64\n",
    "input_size = X.shape[1] # set input_size equal to the width of tensor X\n",
    "output_size = Y.shape[1] # set output_size equal to the width of tensor Y\n",
    "model = PairedDualConvNet(input_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train / test\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size, random_state=42)\n",
    "train_N = int((1 - test_size) * len(X))\n",
    "X_train = X[:train_N]\n",
    "Y_train = Y[:train_N]\n",
    "X_test = X[train_N:]\n",
    "Y_test = Y[train_N:]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca1267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data baseline characteristics as reference for loss \n",
    "mean_y = Y_train.mean(dim=0)\n",
    "std_y = Y_train.std(dim=0)\n",
    "var_y = std_y ** 2\n",
    "var_per_feat = np.var(Y_train.numpy(), axis=0)  # redundant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f44fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 299997\n",
      "Batch size: 64\n",
      "Batches/epoch: 4688\n",
      "Epoch 1/100 — train_loss: 5.147589, val_loss: 3.643855\n",
      "Epoch 2/100 — train_loss: 4.244904, val_loss: 3.484006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 4: Train the Model\n",
    "train_size = len(train_loader.dataset)\n",
    "steps_per_epoch = math.ceil(train_size / batch_size)\n",
    "print(\"Train samples:\", train_size)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Batches/epoch:\", steps_per_epoch)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 100 # max\n",
    "patience = 10\n",
    "best_val = float('inf')\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_train_loss = running_loss / train_size\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, Y_val in test_loader:   # use test_loader or a separate val_loader\n",
    "            Y_val_pred = model(X_val)\n",
    "            l = criterion(Y_val_pred, Y_val)\n",
    "            val_running += l.item() * X_val.size(0)\n",
    "    epoch_val_loss = val_running / len(test_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "    # --- Early stopping ---\n",
    "    if epoch_val_loss < best_val - 1e-9:\n",
    "        best_val = epoch_val_loss\n",
    "        no_improve = 0\n",
    "        # Optionally save best model:\n",
    "        torch.save(model.state_dict(), \"neural_network_pytorch.pth\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs — stopping early at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# After loop: plot train/val loss to inspect convergence\n",
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bbabf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "# model.load_state_dict(torch.load(\"neural_network_pytorch.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.0843\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the Model on Test Data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        total_loss += loss.item() * X_batch.size(0)  # sum up batch loss\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_test_np = Y_test.numpy()\n",
    "with torch.no_grad():\n",
    "    Y_pred_np = model(X_test).numpy()\n",
    "\n",
    "X_all_np = X.numpy()\n",
    "Y_all_np = Y.numpy()\n",
    "with torch.no_grad():\n",
    "    Y_pred_all_np = model(X).numpy()\n",
    "\n",
    "Y_null_all_np = X.numpy()[:, :Y.shape[1]]\n",
    "Y_null_test_np = X_test.numpy()[:, :Y.shape[1]]\n",
    "\n",
    "MSE_per_feat = np.mean((Y_test_np - Y_pred_np) ** 2, axis=0)\n",
    "MSE_per_feat_null = np.mean((Y_test_np - Y_null_test_np) ** 2, axis=0)\n",
    "feats = np.arange(1, Y.shape[1]+1)\n",
    "barwid = .35\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(feats - barwid, var_per_feat, width=barwid, label='Output Variance')\n",
    "plt.bar(feats, MSE_per_feat_null, width=barwid, label='Null MSE')\n",
    "plt.bar(feats + barwid, MSE_per_feat, width=barwid, label='Test MSE')\n",
    "plt.xlabel('Output Feature')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Output Feature Variance vs Test MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations \n",
    "simdur = int(0.2 * fs) # samples \n",
    "plotdomain = 1000 * np.array([-1, 1]) + train_N\n",
    "\n",
    "Ysim = []\n",
    "i0 = plotdomain[0]\n",
    "model.eval()\n",
    "while i0+simdur < plotdomain[1]:\n",
    "    xi = torch.tensor(X_all_np[i0, :].reshape(1, -1), dtype=torch.float32)\n",
    "    for i in range(simdur):\n",
    "        with torch.no_grad():\n",
    "            yi = model(xi).numpy().flatten()\n",
    "        Ysim.append(yi)\n",
    "        # prepare next input\n",
    "        if i < simdur - 1:\n",
    "            event_count_next = X_all_np[i0 + i + 1, -1]  # keep using original event count\n",
    "            xi = torch.tensor(np.hstack([yi, event_count_next]).reshape(1, -1), dtype=torch.float32)\n",
    "    i0 += simdur\n",
    "    print(\"Simulating:\", (i0-plotdomain[0])/(plotdomain[1]-plotdomain[0]), \" complete.\" )\n",
    "\n",
    "Ysim = np.array(Ysim)\n",
    "plotxval = np.arange(len(Ysim)) + plotdomain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show several examples \n",
    "\n",
    "iMSE = np.argsort(MSE_per_feat)\n",
    "iVAR = np.argsort(var_per_feat)\n",
    "iLRN = np.argsort(MSE_per_feat / var_per_feat)\n",
    "iToPlot = [iMSE[:2], iMSE[-2:], iVAR[:2], iVAR[-2:], iLRN[:2], iLRN[-2:]]\n",
    "iToPlot = list(set([i for sublist in iToPlot for i in sublist]))\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "iPlot = 1\n",
    "for i in iToPlot:\n",
    "    plt.subplot(len(iToPlot), 1, iPlot)\n",
    "    plt.plot(Y_all_np[:, i], label='True')\n",
    "    plt.plot(Y_pred_all_np[:, i], label='Predicted', linestyle='--')\n",
    "    plt.plot(Y_null_all_np[:, i], label='Null', linestyle=':')\n",
    "    plt.plot(plotxval, Ysim[:,i], label='Simulated', linestyle='-.')\n",
    "    plt.xlim(plotdomain)\n",
    "\n",
    "    # set the y limits to be slightly larger than the min/max of true values in the plotdomain\n",
    "    y_min = np.min(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_max = np.max(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "    \n",
    "    plt.title(f'Feature {i+1} - MSE: {MSE_per_feat[i]:.4f}, VAR: {var_per_feat[i]:.4f}')\n",
    "    plt.legend(loc='upper right')\n",
    "    iPlot += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
