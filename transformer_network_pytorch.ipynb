{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn import GELU\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37232483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs created: 1439871\n",
      "Input shape : torch.Size([1439871, 128])\n",
      "Output shape: torch.Size([1439871, 7])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the Data ---------------------------------------------------------------------\n",
    "\n",
    "# ------------------------\n",
    "# Load info\n",
    "# ------------------------\n",
    "info_df = pd.read_csv(\"info.csv\")\n",
    "fs = info_df.iloc[0, 5]                  # sampling frequency (Hz)\n",
    "\n",
    "# ------------------------\n",
    "# Load baseline data\n",
    "# ------------------------\n",
    "baseline_df = pd.read_csv(\"baselinedataraw.csv\")\n",
    "baseline_time = baseline_df.iloc[:, 0].values            # time column (seconds)\n",
    "baseline_data = baseline_df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load main data\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"dataraw.csv\")\n",
    "time = df.iloc[:, 0].values            # time column (seconds)\n",
    "data = df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load events\n",
    "# ------------------------\n",
    "events_df = pd.read_csv(\"events.csv\")\n",
    "event_times = events_df.iloc[:, 0].values  # assume first column is event time in seconds\n",
    "event_times = np.sort(event_times)         # ensure sorted\n",
    "\n",
    "# For fast lookup using binary search\n",
    "def count_events_in_window(t, window=0.2):\n",
    "    \"\"\"\n",
    "    Count how many event_times fall in (t - window, t].\n",
    "    Uses bisect for O(log n) search.\n",
    "    \"\"\"\n",
    "    left = bisect.bisect_right(event_times, t - window)\n",
    "    right = bisect.bisect_right(event_times, t)\n",
    "    return right - left\n",
    "\n",
    "# ------------------------\n",
    "# Compute event count for each row\n",
    "# ------------------------\n",
    "event_counts = np.array([count_events_in_window(t, 1.2/fs) for t in time])\n",
    "event_counts = event_counts.reshape(-1, 1)\n",
    "\n",
    "# Append event_counts as an additional input feature\n",
    "data_aug = np.hstack([data, event_counts])\n",
    "# Now each input row has: [original data..., event_count]\n",
    "\n",
    "# ------------------------\n",
    "# Build input-output pairs using the _ ms rule\n",
    "# ------------------------\n",
    "dt_target = 1/fs      # s\n",
    "dt_tol = 0.15 * dt_target\n",
    "drow_target = int(dt_target * fs)  # number of rows \n",
    "\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "# create sliding windows\n",
    "def create_windows(data, seq_len=128, horizon=1):\n",
    "    data = np.asarray(data)\n",
    "    n = len(data) - seq_len - horizon + 1\n",
    "    if n <= 0:\n",
    "        return None, None\n",
    "    # shape = (n, seq_len), stride = (1,1)\n",
    "    x = np.lib.stride_tricks.as_strided(\n",
    "        data,\n",
    "        shape=(n, seq_len),\n",
    "        strides=(data.strides[0], data.strides[0])\n",
    "    ).copy()\n",
    "    # y is simply the future target at +horizon\n",
    "    y = data[seq_len + horizon - 1 : seq_len + horizon - 1 + n].copy()\n",
    "    return x, y\n",
    "\n",
    "# --- baseline data ---\n",
    "Nbl = len(baseline_df)\n",
    "inputs = []\n",
    "for i in range(Nbl - drow_target):\n",
    "    dt = baseline_time[i+drow_target] - baseline_time[i]\n",
    "    if abs(dt - dt_target) <= dt_tol:\n",
    "        inputs.append(baseline_data[i]) \n",
    "    else:\n",
    "        x, y = create_windows(inputs, 128, drow_target)\n",
    "        if x is not None:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        inputs = []\n",
    "# catch trailing segment\n",
    "x, y = create_windows(inputs, 128, drow_target)\n",
    "if x is not None:\n",
    "    X_list.append(x)\n",
    "    Y_list.append(y)\n",
    "\n",
    "X = np.concatenate(X_list, axis=0)\n",
    "Y = np.concatenate(Y_list, axis=0)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "print(\"Pairs created:\", len(X))\n",
    "print(\"Input shape :\", X.shape)   \n",
    "print(\"Output shape:\", Y.shape)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d48702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_model=64, num_heads=4, num_layers=4, dim_ff=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project inputs into model dimension\n",
    "        self.input_proj = nn.Linear(dim_in, dim_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 5000, dim_model))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True  # lets inputs be (B, T, D)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head for next-step prediction\n",
    "        self.fc_out = nn.Linear(dim_model, dim_in)\n",
    "        # TO DO: try changing this to direct one-shot K-step prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, dim_in)\n",
    "        \"\"\"\n",
    "\n",
    "        T = x.size(1)\n",
    "        x = self.input_proj(x) + self.pos_emb[:, :T, :]\n",
    "        z = self.encoder(x)\n",
    "        out = self.fc_out(z[:, -1])  # decode final token\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3189cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "test_size=0.5\n",
    "batch_size = 64\n",
    "\n",
    "model = TimeSeriesTransformer(dim_in=X.shape[-1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train / test\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size, random_state=42)\n",
    "train_N = int((1 - test_size) * len(X))\n",
    "X_train = X[:train_N]\n",
    "Y_train = Y[:train_N]\n",
    "X_test = X[train_N:]\n",
    "Y_test = Y[train_N:]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca1267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data baseline characteristics as reference for loss \n",
    "mean_y = Y_train.mean(dim=0)\n",
    "std_y = Y_train.std(dim=0)\n",
    "var_y = std_y ** 2\n",
    "var_per_feat = np.var(Y_train.numpy(), axis=0)  # redundant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f44fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 719935\n",
      "Batch size: 64\n",
      "Batches/epoch: 11249\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, Y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(Y_pred, Y_batch)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mx: (B, T, dim_in)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m T \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     34\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(z[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# decode final token\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the Model\n",
    "train_size = len(train_loader.dataset)\n",
    "steps_per_epoch = math.ceil(train_size / batch_size)\n",
    "print(\"Train samples:\", train_size)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Batches/epoch:\", steps_per_epoch)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 100 # max\n",
    "patience = 10\n",
    "best_val = float('inf')\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_train_loss = running_loss / train_size\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, Y_val in test_loader:   # use test_loader or a separate val_loader\n",
    "            Y_val_pred = model(X_val)\n",
    "            l = criterion(Y_val_pred, Y_val)\n",
    "            val_running += l.item() * X_val.size(0)\n",
    "    epoch_val_loss = val_running / len(test_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "    # --- Early stopping ---\n",
    "    if epoch_val_loss < best_val - 1e-9:\n",
    "        best_val = epoch_val_loss\n",
    "        no_improve = 0\n",
    "        # Optionally save best model:\n",
    "        # torch.save(model.state_dict(), \"neural_network_pytorch.pth\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs — stopping early at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# After loop: plot train/val loss to inspect convergence\n",
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "# model.load_state_dict(torch.load(\"neural_network_pytorch.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the Model on Test Data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        total_loss += loss.item() * X_batch.size(0)  # sum up batch loss\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_test_np = Y_test.numpy()\n",
    "with torch.no_grad():\n",
    "    Y_pred_np = model(X_test).numpy()\n",
    "\n",
    "X_all_np = X.numpy()\n",
    "Y_all_np = Y.numpy()\n",
    "with torch.no_grad():\n",
    "    Y_pred_all_np = model(X).numpy()\n",
    "\n",
    "Y_null_all_np = X.numpy()[:, :Y.shape[1]]\n",
    "Y_null_test_np = X_test.numpy()[:, :Y.shape[1]]\n",
    "\n",
    "MSE_per_feat = np.mean((Y_test_np - Y_pred_np) ** 2, axis=0)\n",
    "MSE_per_feat_null = np.mean((Y_test_np - Y_null_test_np) ** 2, axis=0)\n",
    "feats = np.arange(1, Y.shape[1]+1)\n",
    "barwid = .35\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(feats - barwid, var_per_feat, width=barwid, label='Output Variance')\n",
    "plt.bar(feats, MSE_per_feat_null, width=barwid, label='Null MSE')\n",
    "plt.bar(feats + barwid, MSE_per_feat, width=barwid, label='Test MSE')\n",
    "plt.xlabel('Output Feature')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Output Feature Variance vs Test MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations \n",
    "simdur = int(0.2 * fs) # samples \n",
    "plotdomain = 1000 * np.array([-1, 1]) + train_N\n",
    "\n",
    "Ysim = []\n",
    "i0 = plotdomain[0]\n",
    "model.eval()\n",
    "while i0+simdur < plotdomain[1]:\n",
    "    xi = torch.tensor(X_all_np[i0, :].reshape(1, -1), dtype=torch.float32)\n",
    "    for i in range(simdur):\n",
    "        with torch.no_grad():\n",
    "            yi = model(xi).numpy().flatten()\n",
    "        Ysim.append(yi)\n",
    "        # prepare next input\n",
    "        if i < simdur - 1:\n",
    "            event_count_next = X_all_np[i0 + i + 1, -1]  # keep using original event count\n",
    "            xi = torch.tensor(np.hstack([yi, event_count_next]).reshape(1, -1), dtype=torch.float32)\n",
    "    i0 += simdur\n",
    "    print(\"Simulating:\", (i0-plotdomain[0])/(plotdomain[1]-plotdomain[0]), \" complete.\" )\n",
    "\n",
    "Ysim = np.array(Ysim)\n",
    "plotxval = np.arange(len(Ysim)) + plotdomain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show several examples \n",
    "\n",
    "iMSE = np.argsort(MSE_per_feat)\n",
    "iVAR = np.argsort(var_per_feat)\n",
    "iLRN = np.argsort(MSE_per_feat / var_per_feat)\n",
    "iToPlot = [iMSE[:2], iMSE[-2:], iVAR[:2], iVAR[-2:], iLRN[:2], iLRN[-2:]]\n",
    "iToPlot = list(set([i for sublist in iToPlot for i in sublist]))\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "iPlot = 1\n",
    "for i in iToPlot:\n",
    "    plt.subplot(len(iToPlot), 1, iPlot)\n",
    "    plt.plot(Y_all_np[:, i], label='True')\n",
    "    plt.plot(Y_pred_all_np[:, i], label='Predicted', linestyle='--')\n",
    "    plt.plot(Y_null_all_np[:, i], label='Null', linestyle=':')\n",
    "    plt.plot(plotxval, Ysim[:,i], label='Simulated', linestyle='-.')\n",
    "    plt.xlim(plotdomain)\n",
    "\n",
    "    # set the y limits to be slightly larger than the min/max of true values in the plotdomain\n",
    "    y_min = np.min(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_max = np.max(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "    \n",
    "    plt.title(f'Feature {i+1} - MSE: {MSE_per_feat[i]:.4f}, VAR: {var_per_feat[i]:.4f}')\n",
    "    plt.legend(loc='upper right')\n",
    "    iPlot += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
