{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn import GELU\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect\n",
    "from myPytorchModels import TimeSeriesTransformer\n",
    "from csv2numpy import prepTimeSeqData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37232483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency (Hz): 1000.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'math' has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the Data ---------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      3\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m  \u001b[38;5;66;03m# sequence length\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m fs, feature_names, Xs, Ys, X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mprepTimeSeqData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m Xs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(Xs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      7\u001b[0m Ys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(Ys, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/Documents/MATLAB/NeuralSystemID/csv2numpy.py:241\u001b[0m, in \u001b[0;36mprepTimeSeqData\u001b[0;34m(dt_target, seq_len, hzn_len, datafile, baselinedatafile, eventsfile, infofile, datarawfile, baselinedatarawfile, maxNumel, drawFromStart, omitOutliers)\u001b[0m\n\u001b[1;32m    238\u001b[0m baseline_data_raw \u001b[38;5;241m=\u001b[39m baseline_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues           \u001b[38;5;66;03m# data columns\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drawFromStart:\n\u001b[0;32m--> 241\u001b[0m     iBLend \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m(baseline_data_raw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], maxPairs)\n\u001b[1;32m    242\u001b[0m     baseline_time \u001b[38;5;241m=\u001b[39m baseline_time[:iBLend]\n\u001b[1;32m    243\u001b[0m     baseline_data_raw \u001b[38;5;241m=\u001b[39m baseline_data_raw[:iBLend, :]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'math' has no attribute 'min'"
     ]
    }
   ],
   "source": [
    "# Prepare the Data ---------------------------------------------------------------------\n",
    "\n",
    "seq_len = 64  # sequence length\n",
    "\n",
    "fs, feature_names, Xs, Ys, X, Y = prepTimeSeqData(seq_len=seq_len)\n",
    "Xs = torch.tensor(Xs, dtype=torch.float32)\n",
    "Ys = torch.tensor(Ys, dtype=torch.float32)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "num_feat = X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3189cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "test_size=0.8\n",
    "batch_size = 32\n",
    "\n",
    "groupsize = 16\n",
    "\n",
    "model = TimeSeriesTransformer(dim_in=num_feat, dim_out=num_feat, time_len=seq_len, group_size=groupsize, num_groups=4)\n",
    "#model = Seq2SeqTimeSeriesTransformer(dim_in=num_feat, dim_out=num_feat, seq_len=seq_len, horizon=hzn_len, dim_model=8, num_heads=2, num_encoder_layers=2, num_decoder_layers=2, dim_ff=16)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# train / test\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size, random_state=42)\n",
    "train_N = int((1 - test_size) * len(X))\n",
    "X_train = X[:train_N]\n",
    "Y_train = Y[:train_N]\n",
    "X_test = X[train_N:]\n",
    "Y_test = Y[train_N:]\n",
    "\n",
    "train_N = int(1 - test_size * len(Xs))\n",
    "Xs_train = Xs[:train_N]\n",
    "Ys_train = Ys[:train_N]\n",
    "Xs_test = Xs[train_N:]\n",
    "Ys_test = Ys[train_N:]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "train_dataset_s = TensorDataset(Xs_train, Ys_train)\n",
    "test_dataset_s = TensorDataset(Xs_test, Ys_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "all_loader = DataLoader(TensorDataset(X, Y), shuffle=False)\n",
    "train_loader_s = DataLoader(train_dataset_s, batch_size, shuffle=True)\n",
    "test_loader_s = DataLoader(test_dataset_s, batch_size, shuffle=False)\n",
    "all_loader_s = DataLoader(TensorDataset(Xs, Ys), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data baseline characteristics as reference for loss \n",
    "mean_y = Y_train.mean(dim=0)\n",
    "std_y = Y_train.std(dim=0)\n",
    "var_y = std_y ** 2\n",
    "var_per_feat = np.var(Y_train.numpy(), axis=0)  # redundant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f44fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# start by performing a few epochs on each set \n",
    "\n",
    "model.train()\n",
    "num_epochs = 5 # max\n",
    "patience = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, Y_val in test_loader:   # use test_loader or a separate val_loader\n",
    "            Y_val_pred = model(X_val)\n",
    "            l = criterion(Y_val_pred, Y_val)\n",
    "            val_running += l.item() * X_val.size(0)\n",
    "    epoch_val_loss = val_running / len(test_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader_s:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_train_loss = running_loss / len(train_loader_s.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, Y_val in test_loader_s:   # use test_loader or a separate val_loader\n",
    "            Y_val_pred = model(X_val)\n",
    "            l = criterion(Y_val_pred, Y_val)\n",
    "            val_running += l.item() * X_val.size(0)\n",
    "    epoch_val_loss = val_running / len(test_loader_s.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "# After loop: plot train/val loss to inspect convergence\n",
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4A: full training on baseline data \n",
    "\n",
    "train_size = len(train_loader.dataset)\n",
    "steps_per_epoch = math.ceil(train_size / batch_size)\n",
    "print(\"Train samples:\", train_size)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Batches/epoch:\", steps_per_epoch)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 100 # max\n",
    "patience = 10\n",
    "best_val = float('inf')\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_train_loss = running_loss / train_size\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, Y_val in test_loader:   # use test_loader or a separate val_loader\n",
    "            Y_val_pred = model(X_val)\n",
    "            l = criterion(Y_val_pred, Y_val)\n",
    "            val_running += l.item() * X_val.size(0)\n",
    "    epoch_val_loss = val_running / len(test_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "    # --- Early stopping ---\n",
    "    if epoch_val_loss < best_val - 1e-9:\n",
    "        best_val = epoch_val_loss\n",
    "        no_improve = 0\n",
    "        # Optionally save best model:\n",
    "        # torch.save(model.state_dict(), \"neural_network_pytorch.pth\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs — stopping early at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# After loop: plot train/val loss to inspect convergence\n",
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load or save model\n",
    "# model.load_state_dict(torch.load(\"neural_network_pytorch.pth\"))\n",
    "# torch.save(model.state_dict(), \"neural_network_pytorch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5A: Evaluate the Model on Test Data\n",
    "\n",
    "Y_pred = []\n",
    "Y_test1 = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        if y_pred.shape[0] == batch_size:\n",
    "            Y_pred.append(y_pred)\n",
    "            Y_test1.append(y_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_np = np.array(Y_pred)\n",
    "Y_pred_np = Y_pred_np.reshape(-1, num_feat)\n",
    "Y_test_np = Y_test.numpy()\n",
    "Y_test1_np = np.array(Y_test1)\n",
    "Y_test1_np = Y_test1_np.reshape(-1, num_feat)\n",
    "\n",
    "Y_null_all_np = X.numpy()[:, -1, :Y.shape[1]]\n",
    "Y_null_test_np = X_test.numpy()[:, -1, :Y.shape[1]]\n",
    "\n",
    "MSE_per_feat = np.mean((Y_test1_np - Y_pred_np) ** 2, axis=0)\n",
    "MSE_per_feat_null = np.mean((Y_test_np - Y_null_test_np) ** 2, axis=0)\n",
    "feats = np.arange(1, Y.shape[1]+1)\n",
    "barwid = .35\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(feats - barwid, var_per_feat, width=barwid, label='Output Variance')\n",
    "plt.bar(feats, MSE_per_feat_null, width=barwid, label='Null MSE')\n",
    "plt.bar(feats + barwid, MSE_per_feat, width=barwid, label='Test MSE')\n",
    "plt.xlabel('Output Feature')\n",
    "plt.xticks(ticks=range(0, len(feats), groupsize), labels=feature_names[::groupsize], rotation=90, ha='right')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Output Feature Variance vs Test MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "MSE_per_feat = MSE_per_feat / np.mean(Y_test1_np**2, axis=0)\n",
    "MSE_per_feat_null = MSE_per_feat_null / np.mean(Y_test_np**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046469d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_np = X.numpy()\n",
    "Y_all_np = Y.numpy()\n",
    "\n",
    "Y_all_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in all_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        Y_all_pred.append(y_pred)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test+Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_all_np = np.array(Y_all_pred)\n",
    "Y_pred_all_np = Y_pred_all_np.reshape(-1, num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations \n",
    "simdur = int(0.2 * fs) # samples \n",
    "plotdomain = 1000 * np.array([-1, 1]) + train_N\n",
    "\n",
    "Ysim = []\n",
    "i0 = plotdomain[0]\n",
    "model.eval()\n",
    "while i0+simdur < plotdomain[1]:\n",
    "    xi = torch.tensor(X_all_np[i0, :, :].reshape(1,-1,num_feat), dtype=torch.float32)\n",
    "    for i in range(simdur):\n",
    "        with torch.no_grad():\n",
    "            yi = model(xi).numpy().flatten()\n",
    "        Ysim.append(yi)\n",
    "        # prepare next input\n",
    "        if i < simdur - 1:\n",
    "            #event_count_next = X_all_np[i0 + i + 1, -1]  # keep using original event count\n",
    "            #xi = torch.tensor(np.hstack([yi, event_count_next]).reshape(1, -1), dtype=torch.float32)\n",
    "            xi = torch.tensor(np.vstack([xi[0, 1:, :], yi]).reshape(1,-1,X.shape[-1]), dtype=torch.float32)\n",
    "    i0 += simdur\n",
    "    print(\"Simulating:\", (i0-plotdomain[0])/(plotdomain[1]-plotdomain[0]), \" complete.\" )\n",
    "\n",
    "Ysim = np.array(Ysim)\n",
    "plotxval = np.arange(len(Ysim)) + plotdomain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show several examples \n",
    "\n",
    "iMSE = np.argsort(MSE_per_feat)\n",
    "iVAR = np.argsort(var_per_feat)\n",
    "iLRN = np.argsort(MSE_per_feat / var_per_feat)\n",
    "iToPlot = [iMSE[:2], iMSE[-2:], iVAR[:2], iVAR[-2:], iLRN[:2], iLRN[-2:]]\n",
    "iToPlot = list(set([i for sublist in iToPlot for i in sublist]))\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "iPlot = 1\n",
    "for i in iToPlot:\n",
    "    plt.subplot(len(iToPlot), 1, iPlot)\n",
    "    plt.plot(Y_all_np[:, i], label='True')\n",
    "    plt.plot(Y_pred_all_np[:, i], label='Predicted', linestyle='--')\n",
    "    plt.plot(Y_null_all_np[:, i], label='Null', linestyle=':')\n",
    "    plt.plot(plotxval, Ysim[:,i], label='Simulated', linestyle='-.')\n",
    "    plt.xlim(plotdomain)\n",
    "\n",
    "    # set the y limits to be slightly larger than the min/max of true values in the plotdomain\n",
    "    y_min = np.min(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_max = np.max(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "    \n",
    "    plt.title(f'Feature {feature_names[i]} - MSE: {MSE_per_feat[i]:.4f}, VAR: {var_per_feat[i]:.4f}')\n",
    "    plt.legend(loc='upper right')\n",
    "    iPlot += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4B: full training on main data \n",
    "\n",
    "train_size = len(train_loader_s.dataset)\n",
    "steps_per_epoch = math.ceil(train_size / batch_size)\n",
    "print(\"Train samples:\", train_size)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Batches/epoch:\", steps_per_epoch)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 100 # max\n",
    "patience = 10\n",
    "best_val = float('inf')\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader_s:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_train_loss = running_loss / train_size\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, Y_val in test_loader_s:   # use test_loader or a separate val_loader\n",
    "            Y_val_pred = model(X_val)\n",
    "            l = criterion(Y_val_pred, Y_val)\n",
    "            val_running += l.item() * X_val.size(0)\n",
    "    epoch_val_loss = val_running / len(test_loader_s.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "    # --- Early stopping ---\n",
    "    if epoch_val_loss < best_val - 1e-9:\n",
    "        best_val = epoch_val_loss\n",
    "        no_improve = 0\n",
    "        # Optionally save best model:\n",
    "        # torch.save(model.state_dict(), \"neural_network_pytorch.pth\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs — stopping early at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# After loop: plot train/val loss to inspect convergence\n",
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e062baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5B: Evaluate the Model on Test Data\n",
    "\n",
    "Y_pred = []\n",
    "Y_test1 = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in test_loader_s:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        if y_pred.shape[0] == batch_size:\n",
    "            Y_pred.append(y_pred)\n",
    "            Y_test1.append(y_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset_s)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_np = np.array(Y_pred)\n",
    "Y_pred_np = Y_pred_np.reshape(-1, num_feat)\n",
    "Y_test_np = Ys_test.numpy()\n",
    "Y_test1_np = np.array(Y_test1)\n",
    "Y_test1_np = Y_test1_np.reshape(-1, num_feat)\n",
    "\n",
    "Y_null_all_np = Xs.numpy()[:, -1, :Ys.shape[1]]\n",
    "Y_null_test_np = Xs_test.numpy()[:, -1, :Ys.shape[1]]\n",
    "\n",
    "MSE_per_feat = np.mean((Y_test1_np - Y_pred_np) ** 2, axis=0)\n",
    "MSE_per_feat_null = np.mean((Y_test_np - Y_null_test_np) ** 2, axis=0)\n",
    "feats = np.arange(1, Ys.shape[1]+1)\n",
    "barwid = .35\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(feats - barwid, var_per_feat, width=barwid, label='Output Variance')\n",
    "plt.bar(feats, MSE_per_feat_null, width=barwid, label='Null MSE')\n",
    "plt.bar(feats + barwid, MSE_per_feat, width=barwid, label='Test MSE')\n",
    "plt.xlabel('Output Feature')\n",
    "plt.xticks(ticks=range(0, len(feats), groupsize), labels=feature_names[::groupsize], rotation=90, ha='right')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Output Feature Variance vs Test MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "MSE_per_feat = MSE_per_feat / np.mean(Y_test1_np**2, axis=0)\n",
    "MSE_per_feat_null = MSE_per_feat_null / np.mean(Y_test_np**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_np = Xs.numpy()\n",
    "Y_all_np = Ys.numpy()\n",
    "\n",
    "Y_all_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in all_loader_s:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        Y_all_pred.append(y_pred)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset_s)\n",
    "    print(f\"Test+Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_all_np = np.array(Y_all_pred)\n",
    "Y_pred_all_np = Y_pred_all_np.reshape(-1, num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c87688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations \n",
    "simdur = int(0.2 * fs) # samples \n",
    "plotdomain = 1000 * np.array([-1, 1]) + train_N\n",
    "\n",
    "Ysim = []\n",
    "i0 = plotdomain[0]\n",
    "model.eval()\n",
    "while i0+simdur < plotdomain[1]:\n",
    "    xi = torch.tensor(X_all_np[i0, :, :].reshape(1,-1,num_feat), dtype=torch.float32)\n",
    "    for i in range(simdur):\n",
    "        with torch.no_grad():\n",
    "            yi = model(xi).numpy().flatten()\n",
    "        Ysim.append(yi)\n",
    "        # prepare next input\n",
    "        if i < simdur - 1:\n",
    "            #event_count_next = X_all_np[i0 + i + 1, -1]  # keep using original event count\n",
    "            #xi = torch.tensor(np.hstack([yi, event_count_next]).reshape(1, -1), dtype=torch.float32)\n",
    "            xi = torch.tensor(np.vstack([xi[0, 1:, :], yi]).reshape(1,-1,Xs.shape[-1]), dtype=torch.float32)\n",
    "    i0 += simdur\n",
    "    print(\"Simulating:\", (i0-plotdomain[0])/(plotdomain[1]-plotdomain[0]), \" complete.\" )\n",
    "\n",
    "Ysim = np.array(Ysim)\n",
    "plotxval = np.arange(len(Ysim)) + plotdomain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca764b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show several examples \n",
    "\n",
    "iMSE = np.argsort(MSE_per_feat)\n",
    "iVAR = np.argsort(var_per_feat)\n",
    "iLRN = np.argsort(MSE_per_feat / var_per_feat)\n",
    "iToPlot = [iMSE[:2], iMSE[-2:], iVAR[:2], iVAR[-2:], iLRN[:2], iLRN[-2:]]\n",
    "iToPlot = list(set([i for sublist in iToPlot for i in sublist]))\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "iPlot = 1\n",
    "for i in iToPlot:\n",
    "    plt.subplot(len(iToPlot), 1, iPlot)\n",
    "    plt.plot(Y_all_np[:, i], label='True')\n",
    "    plt.plot(Y_pred_all_np[:, i], label='Predicted', linestyle='--')\n",
    "    plt.plot(Y_null_all_np[:, i], label='Null', linestyle=':')\n",
    "    plt.plot(plotxval, Ysim[:,i], label='Simulated', linestyle='-.')\n",
    "    plt.xlim(plotdomain)\n",
    "\n",
    "    # set the y limits to be slightly larger than the min/max of true values in the plotdomain\n",
    "    y_min = np.min(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_max = np.max(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "    \n",
    "    plt.title(f'Feature {feature_names[i]} - MSE: {MSE_per_feat[i]:.4f}, VAR: {var_per_feat[i]:.4f}')\n",
    "    plt.legend(loc='upper right')\n",
    "    iPlot += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
