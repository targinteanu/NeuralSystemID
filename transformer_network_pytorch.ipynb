{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn import GELU\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37232483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Data ---------------------------------------------------------------------\n",
    "\n",
    "# ------------------------\n",
    "# Load info\n",
    "# ------------------------\n",
    "info_df = pd.read_csv(\"info.csv\")\n",
    "fs = info_df.iloc[0, 5]                  # sampling frequency (Hz)\n",
    "\n",
    "# ------------------------\n",
    "# Load baseline data\n",
    "# ------------------------\n",
    "baseline_df = pd.read_csv(\"baselinedataraw.csv\")\n",
    "baseline_time = baseline_df.iloc[:, 0].values            # time column (seconds)\n",
    "baseline_data = baseline_df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load main data\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"dataraw.csv\")\n",
    "time = df.iloc[:, 0].values            # time column (seconds)\n",
    "data = df.iloc[:, 1:].values           # data columns\n",
    "\n",
    "# ------------------------\n",
    "# Load events\n",
    "# ------------------------\n",
    "events_df = pd.read_csv(\"events.csv\")\n",
    "event_times = events_df.iloc[:, 0].values  # assume first column is event time in seconds\n",
    "event_times = np.sort(event_times)         # ensure sorted\n",
    "\n",
    "# For fast lookup using binary search\n",
    "def count_events_in_window(t, window=0.2):\n",
    "    \"\"\"\n",
    "    Count how many event_times fall in (t - window, t].\n",
    "    Uses bisect for O(log n) search.\n",
    "    \"\"\"\n",
    "    left = bisect.bisect_right(event_times, t - window)\n",
    "    right = bisect.bisect_right(event_times, t)\n",
    "    return right - left\n",
    "\n",
    "# ------------------------\n",
    "# Compute event count for each row\n",
    "# ------------------------\n",
    "event_counts = np.array([count_events_in_window(t, 1.2/fs) for t in time])\n",
    "event_counts = event_counts.reshape(-1, 1)\n",
    "\n",
    "# Append event_counts as an additional input feature\n",
    "data_aug = np.hstack([data, event_counts])\n",
    "# Now each input row has: [original data..., event_count]\n",
    "\n",
    "# ------------------------\n",
    "# Determine outliers\n",
    "# ------------------------\n",
    "threshsd = 3 # standard deviations \n",
    "threshprop = .5 # proportion of features\n",
    "BLmean = np.mean(baseline_data, axis=0)\n",
    "BLstd = np.std(baseline_data, axis=0)\n",
    "BLisout = np.abs(baseline_data - BLmean) > (threshsd * BLstd)\n",
    "BLisnoise = np.sum(BLisout, axis=1) > (threshprop * baseline_data.shape[1])\n",
    "isout = np.abs(data - BLmean) > (threshsd * BLstd)\n",
    "isnoise = np.sum(isout, axis=1) > (threshprop * data.shape[1])\n",
    "\n",
    "# ------------------------\n",
    "# Build input-output pairs using the _ ms rule\n",
    "# ------------------------\n",
    "dt_target = 1/fs      # s\n",
    "dt_tol = 0.15 * dt_target\n",
    "seq_len = 32                   # samples\n",
    "hzn_len = 16                   # samples\n",
    "drow_target = int(dt_target * fs)  # number of rows \n",
    "\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "# create sliding windows\n",
    "def create_windows(data, seq_len=128, horizon=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - seq_len - horizon + 1):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        Y.append(data[(i+seq_len):(i+seq_len+horizon)])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# --- baseline data ---\n",
    "Nbl = len(baseline_df)\n",
    "inputs = []\n",
    "for i in range(Nbl - drow_target):\n",
    "    dt = baseline_time[i+drow_target] - baseline_time[i]\n",
    "    if (abs(dt - dt_target) <= dt_tol) and (not BLisnoise[i]):\n",
    "        inputs.append(baseline_data[i]) \n",
    "    else:\n",
    "        if len(inputs) > seq_len+hzn_len:\n",
    "            x, y = create_windows(inputs, seq_len, hzn_len)\n",
    "            if x is not None:\n",
    "                X_list.append(x)\n",
    "                Y_list.append(y)\n",
    "            inputs = []\n",
    "# catch trailing segment\n",
    "if len(inputs) > seq_len+hzn_len:\n",
    "    x, y = create_windows(inputs, seq_len, hzn_len)\n",
    "    if x is not None:\n",
    "        X_list.append(x)\n",
    "        Y_list.append(y)\n",
    "\n",
    "X = np.concatenate(X_list, axis=0)\n",
    "Y = np.concatenate(Y_list, axis=0)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "print(\"Pairs created:\", len(X))\n",
    "print(\"Input shape :\", X.shape)   \n",
    "print(\"Output shape:\", Y.shape)\n",
    "\n",
    "num_feat = X.shape[-1]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d48702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture for encoder-only, Hzn=1\n",
    "\n",
    "\n",
    "def elu_feature_map(x):\n",
    "    # FAVOR+ feature map\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # project to Q, K, V\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # reshape to multi-head\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "        # apply kernel feature map\n",
    "        q = elu_feature_map(q)\n",
    "        k = elu_feature_map(k)\n",
    "\n",
    "        # normalize to avoid numerical issues\n",
    "        eps = 1e-6\n",
    "\n",
    "        # Compute KV = sum_t (phi(K_t) * V_t)\n",
    "        # (B, num_heads, head_dim, head_dim)\n",
    "        kv = torch.einsum(\"bthd,bthm->bhmd\", k, v)\n",
    "\n",
    "        # Compute normalizer: z = 1 / (sum_t phi(K_t) * 1)\n",
    "        # (B, num_heads, head_dim)\n",
    "        z = 1 / (torch.einsum(\"bthd,bhd->bth\", q, k.sum(dim=1)) + eps)\n",
    "\n",
    "        # Compute output: y_t = (phi(Q_t) * KV) * z_t\n",
    "        # (B, T, num_heads, head_dim)\n",
    "        out = torch.einsum(\"bthd,bhmd->bthm\", q, kv)\n",
    "        out = out * z.unsqueeze(-1)\n",
    "\n",
    "        # merge heads\n",
    "        out = out.reshape(B, T, D)\n",
    "\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class LinearTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, num_heads, dim_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = LinearAttention(dim_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, dim_model),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        attn_out = self.attn(x)\n",
    "        x = self.norm1(x + self.dropout1(attn_out))\n",
    "\n",
    "        # Feedforward block\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of shape (max_len, dim_model)\n",
    "        pe = torch.zeros(max_len, dim_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "\n",
    "        # Divide by log-based frequencies\n",
    "        div_term = torch.exp(torch.arange(0, dim_model, 2) * (-math.log(10000.0) / dim_model))\n",
    "\n",
    "        # Apply sin to even indices, cos to odd\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register as buffer so it's saved with model but not trainable\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, dim_model)\n",
    "        returns: x + positional_encoding[:, :T, :]\n",
    "        \"\"\"\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_model=64, num_heads=4, num_layers=4, dim_ff=128, pos_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project inputs into model dimension\n",
    "        self.input_proj = nn.Linear(dim_in, dim_model)\n",
    "\n",
    "        # Positional embedding\n",
    "        #self.pos_emb = nn.Parameter(torch.randn(1, pos_len, dim_model))\n",
    "        self.pos_emb = SinusoidalPositionalEncoding(dim_model, max_len=pos_len)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True  # lets inputs be (B, T, D)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head for next-step prediction\n",
    "        self.fc_out = nn.Linear(dim_model, dim_in)\n",
    "        # TO DO: try changing this to direct one-shot K-step prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, dim_in)\n",
    "        \"\"\"\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(f\"Expected input ndim=3, got {x.ndim}\")\n",
    "        T = x.size(1)\n",
    "        #if T > self.pos_emb.size(1):\n",
    "        #    raise RuntimeError(f\"Sequence length T={T} exceeds pos_len={self.pos_emb.size(1)}. \"\n",
    "        #                       \"Either increase pos_len or ensure inputs have smaller T.\")\n",
    "        #x = self.input_proj(x) + self.pos_emb[:, :T, :]\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_emb(x)\n",
    "        z = self.encoder(x)\n",
    "        out = self.fc_out(z[:, -1])  # decode final token\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e928221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture for seq2seq\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: causal mask for decoder\n",
    "# ----------------------------\n",
    "def generate_square_subsequent_mask(sz: int, device: torch.device):\n",
    "    \"\"\"Upper-triangular mask with 0 on and below diagonal, -inf above to mask future tokens.\"\"\"\n",
    "    mask = torch.triu(torch.ones((sz, sz), device=device), diagonal=1).bool()\n",
    "    # nn.Transformer modules expect float mask with -inf for masked positions if using add_mask\n",
    "    # but when using boolean mask arguments (src_key_padding_mask / tgt_key_padding_mask) behavior differs.\n",
    "    # We'll return a float mask suitable for use in `attn_mask` (additive).\n",
    "    attn_mask = torch.full((sz, sz), float('-inf'), device=device)\n",
    "    attn_mask[~mask] = 0.0\n",
    "    return attn_mask  # shape (sz, sz)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Seq2Seq Transformer Model\n",
    "# ----------------------------\n",
    "class Seq2SeqTimeSeriesTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,         # number of input features (all features present in encoder input)\n",
    "        dim_out,        # number of features to predict (subset of dim_in)\n",
    "        seq_len=64,\n",
    "        horizon=32,\n",
    "        dim_model=32,\n",
    "        num_heads=2,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dim_ff=64,\n",
    "        dropout=0.1,\n",
    "        #max_positional_len=5000,\n",
    "        device=torch.device('cpu'),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # 1) Input projection: map raw input features -> model dimension\n",
    "        self.input_proj = nn.Linear(dim_in, dim_model)\n",
    "\n",
    "        # 2) Positional embeddings (learned)\n",
    "        #self.pos_emb_enc = nn.Parameter(torch.randn(1, max_positional_len, dim_model))\n",
    "        #self.pos_emb_dec = nn.Parameter(torch.randn(1, max_positional_len, dim_model))\n",
    "        # sinusoidal positional embeddings\n",
    "        self.pos_emb_enc = SinusoidalPositionalEncoding(dim_model, max_len=seq_len)\n",
    "        self.pos_emb_dec = SinusoidalPositionalEncoding(dim_model, max_len=horizon)\n",
    "\n",
    "        # 3) Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # 4) Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # 5) Output projection: map model dimension -> predicted features\n",
    "        self.output_proj = nn.Linear(dim_model, dim_out)\n",
    "\n",
    "        # optional: small linear to initialize first decoder input from encoder summary, if needed\n",
    "        self.dec_init_proj = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self._reset_parameters()\n",
    "        self.to(device)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Initialization similar to PyTorch Transformer\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def encode(self, src):\n",
    "        \"\"\"\n",
    "        src: (batch, seq_len, dim_in)\n",
    "        returns: memory (batch, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        b, t, _ = src.shape\n",
    "        x = self.input_proj(src)                       # (b, t, dim_model)\n",
    "        #x = x + self.pos_emb_enc[:, :t, :]             # add positional embedding\n",
    "        x = self.pos_emb_enc(x)                        # add positional embedding\n",
    "        memory = self.encoder(x)                       # (b, t, dim_model)\n",
    "        return memory\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: (batch, tgt_len, dim_model)  -- already projected + pos emb\n",
    "        memory: encoder outputs (batch, src_len, dim_model)\n",
    "        \"\"\"\n",
    "        out = self.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "        )  # (b, tgt_len, dim_model)\n",
    "        return out\n",
    "\n",
    "    def forward(self, src, tgt_in):\n",
    "        \"\"\"\n",
    "        Forward pass used in training with teacher forcing.\n",
    "\n",
    "        src: (batch, seq_len, dim_in)\n",
    "        tgt_in: (batch, tgt_len, dim_out) -- ground truth future sequence shifted right\n",
    "                Example: if horizon = H and dim_out=F,\n",
    "                tgt_in[:, 0, :] should be a start token (e.g., zeros)\n",
    "                tgt_in[:, 1:, :] are ground-truth up to H-1 steps (teacher forcing).\n",
    "        Returns:\n",
    "            logits: (batch, tgt_len, dim_out)\n",
    "        \"\"\"\n",
    "        device = src.device\n",
    "        b, src_len, _ = src.shape\n",
    "        _, tgt_len, _ = tgt_in.shape\n",
    "        #assert src_len <= self.pos_emb_enc.shape[1], \"increase max_positional_len\"\n",
    "        #assert tgt_len <= self.pos_emb_dec.shape[1], \"increase max_positional_len\"\n",
    "\n",
    "        # 1) Encode\n",
    "        memory = self.encode(src)  # (b, src_len, dim_model)\n",
    "\n",
    "        # 2) Prepare decoder input embeddings: we project the provided tgt_in (dim_out) into dim_model\n",
    "        #    Common pattern: during training, feed the true previous outputs (teacher forcing).\n",
    "        #    Because tgt_in contains actual feature values (not model embeddings), we map them\n",
    "        #    into model space with a small linear layer (re-using input_proj for simplicity if dims match).\n",
    "        #    Here, to keep model flexible, we'll use a small linear (input_proj_dec) implemented on the fly.\n",
    "        # Simple approach: expand dim_out -> dim_model with a linear\n",
    "        # For simplicity re-use input_proj if dim_in == dim_out; otherwise make a quick linear on the fly:\n",
    "        if self.dim_in == self.dim_out:\n",
    "            # reuse input projection if dims align\n",
    "            tgt_emb = self.input_proj(tgt_in)            # (b, tgt_len, dim_model)\n",
    "        else:\n",
    "            # lightweight linear mapping for decoder inputs (not saved as parameter to keep API simple)\n",
    "            # but better to declare a module if used heavily; here we allocate on the fly for clarity\n",
    "            # (we'll do a proper parametrized projection to avoid re-creating params each forward)\n",
    "            if not hasattr(self, 'tgt_input_proj'):\n",
    "                self.tgt_input_proj = nn.Linear(self.dim_out, self.dim_model).to(self.device)\n",
    "            tgt_emb = self.tgt_input_proj(tgt_in)\n",
    "\n",
    "        # add positional embeddings\n",
    "        #tgt_emb = tgt_emb + self.pos_emb_dec[:, :tgt_len, :]\n",
    "        tgt_emb = self.pos_emb_dec(tgt_emb)\n",
    "\n",
    "        # 3) Create causal mask for decoder self-attention\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_len, device=device)  # (tgt_len, tgt_len)\n",
    "\n",
    "        # 4) Run decoder: decoder expects target embeddings + encoder memory\n",
    "        dec_out = self.decode(\n",
    "            tgt=tgt_emb,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "        )  # (b, tgt_len, dim_model)\n",
    "\n",
    "        # 5) Project decoder outputs to predicted feature space\n",
    "        logits = self.output_proj(dec_out)  # (b, tgt_len, dim_out)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, src, horizon=None, start_token=None):\n",
    "        \"\"\"\n",
    "        Autoregressive generation / inference.\n",
    "\n",
    "        src: (batch, seq_len, dim_in)\n",
    "        horizon: number of steps to generate (defaults to self.horizon)\n",
    "        start_token: (dim_out,) or (batch, dim_out) initial token fed to decoder as t=0.\n",
    "                     If None, zeros are used.\n",
    "\n",
    "        Returns:\n",
    "            generated: (batch, horizon, dim_out)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = src.device\n",
    "        b = src.shape[0]\n",
    "        horizon = self.horizon if horizon is None else horizon\n",
    "\n",
    "        # 1) encode\n",
    "        memory = self.encode(src)  # (b, src_len, dim_model)\n",
    "\n",
    "        # 2) prepare initial decoder input (t=0)\n",
    "        if start_token is None:\n",
    "            cur_input = torch.zeros((b, 1, self.dim_out), device=device)  # (b, 1, dim_out)\n",
    "        else:\n",
    "            # allow vector or batch vector\n",
    "            st = torch.tensor(start_token, device=device, dtype=src.dtype)\n",
    "            if st.dim() == 1:\n",
    "                st = st.unsqueeze(0).expand(b, -1)\n",
    "            cur_input = st.unsqueeze(1)  # (b, 1, dim_out)\n",
    "\n",
    "        generated = []\n",
    "        # autoregressive loop\n",
    "        for t in range(horizon):\n",
    "            # build tgt_in as the sequence of already generated tokens for this batch\n",
    "            if len(generated) == 0:\n",
    "                tgt_in = cur_input  # (b, 1, dim_out)\n",
    "            else:\n",
    "                # stack previously generated tokens\n",
    "                prev = torch.cat(generated, dim=1)  # (b, t, dim_out)\n",
    "                tgt_in = torch.cat([cur_input, prev], dim=1)  # (b, t+1, dim_out)\n",
    "\n",
    "            # forward through model (teacher forcing not applied)\n",
    "            logits = self.forward(src, tgt_in)  # (b, t+1, dim_out)\n",
    "            # take the last step's predictions as next token\n",
    "            next_token = logits[:, -1:, :]  # (b, 1, dim_out)\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # note: optional sampling or temperature can be applied here for stochastic outputs\n",
    "\n",
    "        gen = torch.cat(generated, dim=1)  # (b, horizon, dim_out)\n",
    "        return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3189cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "test_size=0.7\n",
    "batch_size = 32\n",
    "\n",
    "model = TimeSeriesTransformer(dim_in=num_feat, dim_model=8, num_heads=2, num_layers=2, dim_ff=16, pos_len=seq_len)\n",
    "#model = Seq2SeqTimeSeriesTransformer(dim_in=num_feat, dim_out=num_feat, seq_len=seq_len, horizon=hzn_len, dim_model=8, num_heads=2, num_encoder_layers=2, num_decoder_layers=2, dim_ff=16)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train / test\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size, random_state=42)\n",
    "train_N = int((1 - test_size) * len(X))\n",
    "X_train = X[:train_N]\n",
    "Y_train = Y[:train_N]\n",
    "X_test = X[train_N:]\n",
    "Y_test = Y[train_N:]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "all_loader = DataLoader(TensorDataset(X, Y), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data baseline characteristics as reference for loss \n",
    "mean_y = Y_train.mean(dim=0)\n",
    "std_y = Y_train.std(dim=0)\n",
    "var_y = std_y ** 2\n",
    "var_per_feat = np.var(Y_train.numpy(), axis=0)  # redundant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f44fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Model\n",
    "train_size = len(train_loader.dataset)\n",
    "steps_per_epoch = math.ceil(train_size / batch_size)\n",
    "print(\"Train samples:\", train_size)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Batches/epoch:\", steps_per_epoch)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 100 # max\n",
    "patience = 10\n",
    "best_val = float('inf')\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_train_loss = running_loss / train_size\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, Y_val in test_loader:   # use test_loader or a separate val_loader\n",
    "            Y_val_pred = model(X_val)\n",
    "            l = criterion(Y_val_pred, Y_val)\n",
    "            val_running += l.item() * X_val.size(0)\n",
    "    epoch_val_loss = val_running / len(test_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — train_loss: {epoch_train_loss:.6f}, val_loss: {epoch_val_loss:.6f}\")\n",
    "\n",
    "    # --- Early stopping ---\n",
    "    if epoch_val_loss < best_val - 1e-9:\n",
    "        best_val = epoch_val_loss\n",
    "        no_improve = 0\n",
    "        # Optionally save best model:\n",
    "        # torch.save(model.state_dict(), \"neural_network_pytorch.pth\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs — stopping early at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# After loop: plot train/val loss to inspect convergence\n",
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "# model.load_state_dict(torch.load(\"neural_network_pytorch.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the Model on Test Data\n",
    "\n",
    "Y_pred = []\n",
    "Y_test1 = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        if y_pred.shape[0] == seq_len:\n",
    "            Y_pred.append(y_pred)\n",
    "            Y_test1.append(y_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_np = np.array(Y_pred)\n",
    "Y_pred_np = Y_pred_np.reshape(-1, num_feat)\n",
    "Y_test_np = Y_test.numpy()\n",
    "Y_test1_np = np.array(Y_test1)\n",
    "Y_test1_np = Y_test1_np.reshape(-1, num_feat)\n",
    "\n",
    "Y_null_all_np = X.numpy()[:, -1, :Y.shape[1]]\n",
    "Y_null_test_np = X_test.numpy()[:, -1, :Y.shape[1]]\n",
    "\n",
    "MSE_per_feat = np.mean((Y_test1_np - Y_pred_np) ** 2, axis=0)\n",
    "MSE_per_feat_null = np.mean((Y_test_np - Y_null_test_np) ** 2, axis=0)\n",
    "feats = np.arange(1, Y.shape[1]+1)\n",
    "barwid = .35\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(feats - barwid, var_per_feat, width=barwid, label='Output Variance')\n",
    "plt.bar(feats, MSE_per_feat_null, width=barwid, label='Null MSE')\n",
    "plt.bar(feats + barwid, MSE_per_feat, width=barwid, label='Test MSE')\n",
    "plt.xlabel('Output Feature')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Output Feature Variance vs Test MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046469d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_np = X.numpy()\n",
    "Y_all_np = Y.numpy()\n",
    "\n",
    "Y_all_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in all_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        total_loss += loss.item() * x_batch.size(0)  # sum up batch loss\n",
    "        Y_all_pred.append(y_pred)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    print(f\"Test+Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "Y_pred_all_np = np.array(Y_all_pred)\n",
    "Y_pred_all_np = Y_pred_all_np.reshape(-1, num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations \n",
    "simdur = int(0.2 * fs) # samples \n",
    "plotdomain = 1000 * np.array([-1, 1]) + train_N\n",
    "\n",
    "Ysim = []\n",
    "i0 = plotdomain[0]\n",
    "model.eval()\n",
    "while i0+simdur < plotdomain[1]:\n",
    "    xi = torch.tensor(X_all_np[i0, :, :].reshape(1,-1,num_feat), dtype=torch.float32)\n",
    "    for i in range(simdur):\n",
    "        with torch.no_grad():\n",
    "            yi = model(xi).numpy().flatten()\n",
    "        Ysim.append(yi)\n",
    "        # prepare next input\n",
    "        if i < simdur - 1:\n",
    "            #event_count_next = X_all_np[i0 + i + 1, -1]  # keep using original event count\n",
    "            #xi = torch.tensor(np.hstack([yi, event_count_next]).reshape(1, -1), dtype=torch.float32)\n",
    "            xi = torch.tensor(np.vstack([xi[0, 1:, :], yi]).reshape(1,-1,X.shape[-1]), dtype=torch.float32)\n",
    "    i0 += simdur\n",
    "    print(\"Simulating:\", (i0-plotdomain[0])/(plotdomain[1]-plotdomain[0]), \" complete.\" )\n",
    "\n",
    "Ysim = np.array(Ysim)\n",
    "plotxval = np.arange(len(Ysim)) + plotdomain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show several examples \n",
    "\n",
    "iMSE = np.argsort(MSE_per_feat)\n",
    "iVAR = np.argsort(var_per_feat)\n",
    "iLRN = np.argsort(MSE_per_feat / var_per_feat)\n",
    "iToPlot = [iMSE[:2], iMSE[-2:], iVAR[:2], iVAR[-2:], iLRN[:2], iLRN[-2:]]\n",
    "iToPlot = list(set([i for sublist in iToPlot for i in sublist]))\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "iPlot = 1\n",
    "for i in iToPlot:\n",
    "    plt.subplot(len(iToPlot), 1, iPlot)\n",
    "    plt.plot(Y_all_np[:, i], label='True')\n",
    "    plt.plot(Y_pred_all_np[:, i], label='Predicted', linestyle='--')\n",
    "    plt.plot(Y_null_all_np[:, i], label='Null', linestyle=':')\n",
    "    plt.plot(plotxval, Ysim[:,i], label='Simulated', linestyle='-.')\n",
    "    plt.xlim(plotdomain)\n",
    "\n",
    "    # set the y limits to be slightly larger than the min/max of true values in the plotdomain\n",
    "    y_min = np.min(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_max = np.max(Y_all_np[plotdomain[0]:plotdomain[1], i])\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "    \n",
    "    plt.title(f'Feature {i+1} - MSE: {MSE_per_feat[i]:.4f}, VAR: {var_per_feat[i]:.4f}')\n",
    "    plt.legend(loc='upper right')\n",
    "    iPlot += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
